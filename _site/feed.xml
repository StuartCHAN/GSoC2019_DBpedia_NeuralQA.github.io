<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Stuart Chen's Blog</title>
    <description> L'air immense ouvre et referme mon livre, la vague en poudre ose jaillir des rocs! </description>
    <link>https://stuartchan.github.io/</link>
    <atom:link href="https://stuartchan.github.io/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Sat, 31 Aug 2019 11:37:32 +0800</pubDate>
    <lastBuildDate>Sat, 31 Aug 2019 11:37:32 +0800</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>Week 11&amp;12</title>
        <description>&lt;h1 id=&quot;summing-up&quot;&gt;Summing Up&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;project-outline&quot;&gt;Project Outline&lt;/h2&gt;

&lt;h3 id=&quot;week-1112&quot;&gt;Week 11&amp;amp;12&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Wiki Extraction &amp;amp; Sentences Filtering&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;week-910&quot;&gt;Week 9&amp;amp;10&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Univerasal Sentence Encoder for Template Matching &amp;amp; Templates Bank&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;week-78&quot;&gt;Week 7&amp;amp;8&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Analyses for Previous Works&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;week-6&quot;&gt;Week 6&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Vector Similarity Calculation&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;week-5&quot;&gt;Week 5&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Issues Analyses&lt;/li&gt;
  &lt;li&gt;Vectors Embedding&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;week-4&quot;&gt;Week 4&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Pre-evaluation Preparation&lt;/li&gt;
  &lt;li&gt;Exploring Potential Issues for Current NSpM&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;week-3&quot;&gt;Week 3&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Improving the Generation Component&lt;/li&gt;
  &lt;li&gt;Thinking About The Evaluations&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;week-2&quot;&gt;Week 2&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Entity Recognition&lt;/li&gt;
  &lt;li&gt;Template Generation&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;week-1&quot;&gt;Week 1&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;qald evaluation against GERBIL interface building the semantic tree structue for the natural lang.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;evaluation-of-the-project&quot;&gt;Evaluation of the Project&lt;/h2&gt;

&lt;p&gt;In the initial period, we wanted to use DBpedia embedding to do the SQuAD machine reading comprehension tasks with reinforcement learning, but gradually we realize the performance of the neural SPARQL machine is highly dependent on the training data which indicate the crucial necessity of automating the templates generation from long contextual passages. The Wikipedia is a wonderful source of plenty of such articles relevant to DBpedia RDF triples, so we decided to evolve an intelligent neural SPARQL machine with automated templates generation, comparison, and accumulation to try to approach a never-ending-learning intelligent agent.&lt;/p&gt;

&lt;p&gt;Of course, during the coding, we have countered so many difficulties, like doing the benchmark evaluations and some tough impediments, but as now I think about these problems, I think they gave me a totally thorough growth. I got to learn more and more about the newest products in the industry and get more adequate with the international coding standards which open my door to a bigger world. For example, in the part of calculating the vector similarity to match existing templates, we first used word mover distance with GLoVe vectors via gensim, but we found that was too heavy and too slow, then we used spaCy and found it much speedier. And soon after this, we found the Universal Sentence Encoder is even better in this task, which is a huge evolution in our development.&lt;/p&gt;

&lt;p&gt;Another thing that I still remember is the paraphrasing of the predicates, we used to think load all the phrases in RAM and do the matching. I still remember that file was so huge even more than 17.6 GB. Then I found the wordnet from nltk can accomplish this paraphrasing task without such a huge cost, which is a smart solution.&lt;/p&gt;

&lt;h3 id=&quot;future-works&quot;&gt;Future Works&lt;/h3&gt;

&lt;p&gt;We hope to keep on the work on making the question generation even better and including ASK queries, queries that require filter (how many, how much, etc.) and complex queries as well. Because we believe this can make the neural SPARQL machines get even better and better performance.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;To read more, please refer to the &lt;a href=&quot;https://github.com/StuartCHAN/neural-qa#summary&quot;&gt;repository summary&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 31 Aug 2019 00:00:00 +0800</pubDate>
        <link>https://stuartchan.github.io/2019/08/31/Summing-Up/</link>
        <guid isPermaLink="true">https://stuartchan.github.io/2019/08/31/Summing-Up/</guid>
        
        <category>GSoC2019</category>
        
        <category>NLP</category>
        
        <category>DBpedia</category>
        
        
      </item>
    
      <item>
        <title>Week 11&amp;12</title>
        <description>&lt;h2 id=&quot;preface&quot;&gt;Preface&lt;/h2&gt;

&lt;p&gt;We focus on:&lt;/p&gt;

&lt;p&gt;1) get the natural language materials from the Wikipedia page for templates generation;&lt;/p&gt;

&lt;p&gt;2) automate the paraphrases for predicates.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;1-wiki-extraction--sentences-filtering&quot;&gt;1. Wiki Extraction &amp;amp; Sentences Filtering&lt;/h2&gt;

&lt;h3 id=&quot;11-wikipedia-articles-extraction&quot;&gt;1.1 Wikipedia Articles Extraction&lt;/h3&gt;

&lt;p&gt;The Wiki pages are the excellent resources for us to get the natural language texts that containing the RDFs in DBpedia.&lt;/p&gt;

&lt;p&gt;We use the library called &lt;a href=&quot;https://github.com/martin-majlis/Wikipedia-API&quot;&gt;Wikipedia-API&lt;/a&gt; to use Python wrapper for Wikipedias’ API, which facilitates extracting texts, sections, links, categories, translations, etc from the pages of Wikipedia. Documentation provides code snippets for the most common use cases.&lt;/p&gt;

&lt;p&gt;For example, a ‘dbr’ should be like: ‘Audrey_Hepburn’, which is the identifier of both the DBPeida entity and the Wiki page.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;extract_page&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dbr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# 
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;#wiki_wiki = wikipediaapi.Wikipedia( language='en', extract_format=wikipediaapi.ExtractFormat.WIKI )    
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;wiki_wiki&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wikipediaapi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Wikipedia&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'en'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;page_wiki&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wiki_wiki&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;page&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dbr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;#page_wiki = wiki_wiki.page('Audrey_Hepburn')    
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot; Page - Exists: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;page_wiki&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exists&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Page - Exists: True
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;page_wiki&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exists&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fulltext&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;page_wiki&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;savepath&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;save_page&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dbr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fulltext&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;savepath&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;12--preprocesing&quot;&gt;1.2  Preprocesing&lt;/h3&gt;

&lt;p&gt;We then use the spaCy to do the sentences segmentation. When using a space-based pre-trained model, the sentences are split based on training data provided during the model’s training procedure.&lt;/p&gt;

&lt;p&gt;And, we also remove the puntuations except the comma in the sentences, because we need to use the comma to decompose the senetences in the part of converting them into questions.&lt;/p&gt;

&lt;p&gt;Then we calculate the average length of the sentences we get and set it as the treshold to filter out those sentences that are too long.&lt;/p&gt;

&lt;h2 id=&quot;2-predicate-paraphrasing--rdf-matching&quot;&gt;2. Predicate-paraphrasing &amp;amp; RDF Matching&lt;/h2&gt;

&lt;h3 id=&quot;21-coreference-resolution&quot;&gt;2.1. Coreference Resolution&lt;/h3&gt;

&lt;p&gt;Let’s see what is the coreference resolution.&lt;/p&gt;

&lt;p&gt;This’s an explanative example(by StandfordNLP) [1],&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://res.cloudinary.com/stuarteec/image/upload/v1567214556/corefexample.StandfordNLPpng_gpvkbs.png&quot; alt=&quot;Coreference Resolution&quot; title=&quot;Coreference Resolution(by StandfordNLP)&quot; /&gt;&lt;/p&gt;

&lt;p&gt;so, when the lady stated as above for this man’s election, we can see the corefernce relations as below:&lt;/p&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    THE LADY: [ I, my, She ]
    THE MAN:  [ Nader, he ]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This can clarify what the words or pronouns are talking about in the passage: to which entity? from which? who? what’s this entity/pronoun?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;in this example,&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;   his wife Michelle &lt;/code&gt;&lt;/p&gt;

&lt;p&gt;through the Coreference Resolution, we can figure out that &lt;code class=&quot;highlighter-rouge&quot;&gt;whom does it mean by 'his' ? &lt;/code&gt; in the context, which helps a lot in the RDF entities confirmation.&lt;/p&gt;

&lt;p&gt;Then we get the RDF:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;'his' &amp;lt;dbr:Barack_Obama&amp;gt;&lt;/code&gt; –&amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;'wife' &amp;lt;dbo:spouse&amp;gt;&lt;/code&gt; –&amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;'Michelle' &amp;lt;dbr:Michelle_Obama&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Resolution of coreference is a well-studied problem in computer linguistics discourse. To obtain the correct interpretation of a text, or even to assess the relative significance of the different topics listed above, pronouns and other reference phrases must be related to the right individuals. Usually, algorithms designed to solve coreferences first look for the closestpre.&lt;/p&gt;

&lt;p&gt;`Coreference Resolution in spaCy with Neural Networks.’&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/huggingface/neuralcoref&quot;&gt;NeuralCoref&lt;/a&gt;[2] is a spaCy 2.1 + pipeline extension that uses a neural network to annotate and resolve coreference clusters. NeuralCoref is ready for production, incorporated into the NLP pipeline of spaCy and can be extended to fresh training datasets.&lt;/p&gt;

&lt;p&gt;It solves the problem of pinpointing the pronounces and indicating the right corefernce of an entity.&lt;/p&gt;

&lt;h3 id=&quot;22-automatic-predicate-paraphrasing&quot;&gt;2.2. Automatic Predicate-paraphrasing&lt;/h3&gt;

&lt;p&gt;At first, we use the API of an online paraphraser to do the part of automatic paraphrasing, which we found that might not be suitable to do the paraphrasing of short phrases like the predicates.&lt;/p&gt;

&lt;p&gt;Then, we use the wordnet[3] via NLTK to take the paraphrasing task.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nltk.corpus&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wordnet&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;WordNet is the lexical database, equivalent to kind of English language dictionary, specifically intended for the processing of natural language.&lt;/p&gt;

&lt;p&gt;Synset is a unique kind of easy interface to look up phrases in WordNet that is present in NLTK. Synset instances are synonymous word groupings expressing the same concept. Some of the phrases have just one synset, while others have several.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;n&quot;&gt;syn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wordnet&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;synsets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'friend'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Synset name :  &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;syn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; 

&lt;span class=&quot;c1&quot;&gt;# Defining the word 
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Synset meaning : &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;syn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;definition&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; 

&lt;span class=&quot;c1&quot;&gt;# list of phrases that use the word in context 
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Synset example : &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;syn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;examples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Synset name :   friend.n.01

Synset meaning :  a person you know well and regard with affection and trust

Synset example :  &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'he was my best friend at the university'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To understand how the paraphraser works, we need to konw more about the &lt;code class=&quot;highlighter-rouge&quot;&gt;Hypernyms&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;Hyponyms&lt;/code&gt; :&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Hypernyms&lt;/code&gt; are the more abstract terms for the entity that a word describs.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Hyponyms&lt;/code&gt; are the specific terms, we can say that they are the subclasses of the entity.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Synset abstract term :  &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;syn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hypernyms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; 

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Synset specific term :  &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;syn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hypernyms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hyponyms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; 

&lt;span class=&quot;n&quot;&gt;syn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;root_hypernyms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; 
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Synset root hypernerm :  &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;syn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;root_hypernyms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Synset name :   friend.n.01

Synset abstract term :   &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Synset&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'person.n.01'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)]&lt;/span&gt;

Synset specific term :   &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Synset&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'abator.n.01'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;, Synset&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'abjurer.n.01'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;, Synset&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'abomination.n.01'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;, Synset&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'abstainer.n.02'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;, Synset&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'achiever.n.01'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)]&lt;/span&gt;

Synset root hypernerm :   &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Synset&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'entity.n.01'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;What is the Hypernyms and Hyponyms distinction?&lt;/p&gt;

&lt;p&gt;Hyponymy indicates the connection between a generic (hypernyme) word and a particular (hyponymous) term.&lt;/p&gt;

&lt;p&gt;A hyponym is a word or sentence that has a more particular semantic field than its hypernym.&lt;/p&gt;

&lt;p&gt;A hypernym’s semantic field, also known as a superordinate, is wider than a hyponym’s.&lt;/p&gt;

&lt;p&gt;Then we can get:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;ps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PorterStemmer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; 

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;paraphrase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;synsets&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wordnet&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;synsets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;#synonyms = [[[lemma.name() for lemma in synonym.lemmas()] for synonym in synset.hyponyms()] for synset in synsets ]
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;synonyms&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lemma&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lemma&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;synonym&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lemmas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;synonym&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;synsets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hyponyms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()]&lt;/span&gt;  
    &lt;span class=&quot;n&quot;&gt;synonymset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;synonym&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;synonyms&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;syn&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;synonym&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;syn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;syn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'_'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;' '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;syn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;syn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ps&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stem&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;syn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;synonymset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;syn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;#synonymset.append(word)
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;synonymset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;synonymset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;synonymset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;insert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;synonymset&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then we can do the RDF matching, by given a triple of DBpedia RDF, 
 e.g. &lt;a href=&quot;http://dbpedia.org/resource/Barack_Obama&quot;&gt;http://dbpedia.org/resource/Barack_Obama&lt;/a&gt;	&lt;a href=&quot;http://dbpedia.org/property/parents&quot;&gt;http://dbpedia.org/property/parents&lt;/a&gt;	&lt;a href=&quot;http://dbpedia.org/resource/Barack_Obama_Sr.&quot;&gt;http://dbpedia.org/resource/Barack_Obama_Sr.&lt;/a&gt; .&lt;/p&gt;

&lt;p&gt;we can have&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;{&quot;subject&quot;:CoreferenceResolution(dbr_Barack_Obama), &quot;predicate&quot;:dbp_parents+synonyms(&quot;parents&quot;), &quot;obeject&quot;:dbr_Barack_Obama_Sr}&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;to match the sentences that are sitable for this RDF triple.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] Coreference Resolution, StandfordNLP, https://nlp.stanford.edu/projects/coref.shtml&lt;/p&gt;

&lt;p&gt;[2] NeuralCoref, https://github.com/huggingface/neuralcoref&lt;/p&gt;

&lt;p&gt;[3] WordNet, https://wordnet.princeton.edu/&lt;/p&gt;

</description>
        <pubDate>Sun, 18 Aug 2019 00:00:00 +0800</pubDate>
        <link>https://stuartchan.github.io/2019/08/18/Week-11&12/</link>
        <guid isPermaLink="true">https://stuartchan.github.io/2019/08/18/Week-11&12/</guid>
        
        <category>GSoC2019</category>
        
        <category>NLP</category>
        
        <category>DBpedia</category>
        
        
      </item>
    
      <item>
        <title>Week 9&amp;10</title>
        <description>&lt;h2 id=&quot;preface&quot;&gt;Preface&lt;/h2&gt;

&lt;p&gt;We focus on:&lt;/p&gt;

&lt;p&gt;1) classifying the new questions by the vector similarity matching based on univeral sentence encoder;&lt;/p&gt;

&lt;p&gt;2) constructure of the Template Bank.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;1-templates-matching&quot;&gt;1. Templates Matching&lt;/h2&gt;

&lt;h3 id=&quot;11-universal-sentence-encoder&quot;&gt;1.1 Universal Sentence Encoder&lt;/h3&gt;

&lt;p&gt;Why we need it?&lt;/p&gt;

&lt;p&gt;Considering the great human efforts we have expended in manually composing the templates, we believe we can make better use of them for generating more new templates,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;for example, when we see these two questions,&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;who is the wife of Obama?&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;who is the spouse of Obama?&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;actually, they can be represented by the same query for the same semantic meaning.&lt;/p&gt;

&lt;p&gt;Then, how to accomplish the semantic searching?&lt;/p&gt;

&lt;p&gt;Yes, we can use the similarity of embedding vectors by the Universal Sentence Encoder.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://arxiv.org/abs/1803.11175&quot;&gt;Universal Sentence Encoder&lt;/a&gt; [1] encodes text into vectors of high dimensions that can be used in classifying documents, calculating semantic similarity, topics clustering, and more other natural language processing tasks. With &lt;a href=&quot;https://tfhub.dev/google/universal-sentence-encoder&quot;&gt;Tensorflow-hub&lt;/a&gt;, we deploy the Universal Sentence Encoder to calculate the similarity between the new question and the existing templates to do the matching task.&lt;/p&gt;

&lt;p&gt;The encoder part comes with two variant options, one with &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;Transformer&lt;/a&gt; [2] encoder and the other with &lt;a href=&quot;http://cs.umd.edu/~miyyer/pubs/2015_acl_dan.pdf&quot;&gt;Deep Averaging Network&lt;/a&gt; (DAN) [3]. Both having a necessity for precision and computational resource trading, while the one with the Transformer encoder is of higher accuracy at a cost of computing, it is more intensive in computational terms. And, the other with DNA encoding is less costly computationally but with less precision.&lt;/p&gt;

&lt;p&gt;Here we use the Transformer-based sentence embedding to match the question and its closet template.&lt;/p&gt;

&lt;p&gt;Example,&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;__________________________________________________________________________________________________
Layer &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;                    Output Shape         Param &lt;span class=&quot;c&quot;&gt;#     Connected to                     &lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;==================================================================================================&lt;/span&gt;
input_2 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;InputLayer&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;            &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;None, 512&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;          0                                            
__________________________________________________________________________________________________
input_3 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;InputLayer&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;            &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;None, 512&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;          0                                            
__________________________________________________________________________________________________
model_1 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Model&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;                 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;None, 512&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;          262656      input_2[0][0]                    
                                                                 input_3[0][0]                    
__________________________________________________________________________________________________
lambda_1 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Lambda&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;None, 512&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;          0           model_1[1][0]                    
                                                                 model_1[2][0]                    
__________________________________________________________________________________________________
dense_2 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Dense&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;                 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;None, 1&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;            513         lambda_1[0][0]                   
&lt;span class=&quot;o&quot;&gt;==================================================================================================&lt;/span&gt;
Total params: 263,169
Trainable params: 263,169
Non-trainable params: 0
__________________________________________________________________________________________________
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So, if we compare &lt;code class=&quot;highlighter-rouge&quot;&gt;who is the wife of Obama?&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;who is the spouse of Obama?&lt;/code&gt;, they would be classified into the same semantic meaning by setting the threshold similarity score.&lt;/p&gt;

&lt;p&gt;However, when we faced &lt;code class=&quot;highlighter-rouge&quot;&gt;who is the father of Obama?&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;who is the spouse of Obama?&lt;/code&gt;, the difference will be shown in the similarity score. If the threshold is lower than what’s required, we call the template composing function then.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;12-preprocesing&quot;&gt;1.2 Preprocesing&lt;/h3&gt;

&lt;p&gt;The templateset must first be filtered out the stop-words, the punctuations, and then turn into lower case for data regularization.&lt;/p&gt;

&lt;h3 id=&quot;13-detecting-the-class-of-the-question&quot;&gt;1.3 Detecting The Class Of The Question&lt;/h3&gt;

&lt;p&gt;We use the DBpedia Spotlight API to detect the belonging class of the noun subject in the question, which helps to classify the question into its correspondent templateset.&lt;/p&gt;

&lt;p&gt;Why it’s necessary? Because, the sentences get confused or not depends on how close the two classes that they belong to, like the dbo:SportsLeague and dbo:Monument, their topics are not so similar, so they’re not easy to get confused, but when I took the&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;dbo:Place versus dbo:Monument, the &quot;dbo:Place;;;what is the address of &amp;lt;X&amp;gt;;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and the&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; dbo:Monument;;;what is the location of &amp;lt;A&amp;gt;;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;this case could be easily confused.&lt;/p&gt;

&lt;p&gt;In human concepts, the “address” and the “location” might holding the same meaning, while it is not that case in knowledge graph of DBpedia:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sparql&quot;&gt;dbo:Place;;;what is the address of &amp;lt;X&amp;gt;;SELECT ?x WHERE { &amp;lt;X&amp;gt; &amp;lt;http://dbpedia.org/ontology/address&amp;gt; ?x };SELECT ?a WHERE { ?a &amp;lt;http://dbpedia.org/ontology/address&amp;gt; [] . ?a a &amp;lt;http://dbpedia.org/ontology/Place&amp;gt; }
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-sparql&quot;&gt;dbo:Monument;;;what is the location of &amp;lt;A&amp;gt;;select ?a where { &amp;lt;A&amp;gt; dbo:location ?a };select distinct(?a) where { ?a dbo:location [] }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So, for promising the accuracy, we should classify the Classes before matching, like, before classifying these two above, we first classify the new question into a Class then conduct the matching.&lt;/p&gt;

&lt;p&gt;It’s not so straightforward that it requires to find the entity type first:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Statue of Liberty --&amp;gt; Place or Monument
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;However, not every entity belongs to a single upper Class. Some certain entities might belong to multiple Classes, like in&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&quot;which television show were created by joe austen?&quot; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;the&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&quot;joe austen&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;is annotated as&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&quot;dbr:Joe_Austen&quot; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;with three upper Classes,&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;dbo:Person, dbo:Artist, dbo:Agent, 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and the Spotlight can also indicate the most probable Class in this context is the “dbo:Person” in the return. So, to know to which Class it belongs, we can respectively fetch their lists of properties that its three upper Classes have,  and see in which list of properties the predicate exists.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;14--semantic-searching&quot;&gt;1.4  Semantic Searching&lt;/h3&gt;

&lt;p&gt;How do we use the Universal Sentence Encoder to build the Semantic Searching?&lt;/p&gt;

&lt;p&gt;Given the detected Class and its belonging templateset, we can use the similarity searching function to get the matched template for the question:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;semantic_search&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;texts_processed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vectors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;query&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;preprocess_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; Extracting features...&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;query_vec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ravel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;texts_processed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;qvec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vectors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ravel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;sim&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cosine_similarity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query_vec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;qvec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        
    &lt;span class=&quot;nb&quot;&gt;sorted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reverse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Sorting for getting the most matched one.
&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;res&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;semantic_search&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;This is a new sentence. &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;texts_processed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BASE_VECTORS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;2-templates-bank&quot;&gt;2. Templates Bank&lt;/h2&gt;

&lt;h3 id=&quot;21-the-directory-for-the-bank&quot;&gt;2.1 The Directory For The Bank&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;-- Bank
    -- Class 0
        -- class0.csv
        -- class0.vectors
    -- Class 1
        -- class1.csv 
        -- class1.vectors
    -- Class 2
        -- class2.csv
        -- class2.vectors
    -- Class ...

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;In this directory, the Bank consists of the Classes of DBpedia ontologies, and each Class has its own folder. Its folder contains the templateset CSV and its sentences vectors set.&lt;/p&gt;

&lt;p&gt;We use this function to see the existing Classes,&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;csv&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pickle&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;../data/Bank&quot;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#the Bank directory is located in the neural-qa/data
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;files&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;listdir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#to get all the files/folders names in the dir
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;templates_pool&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;file&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;files&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#iterate to get the folders
&lt;/span&gt;
     &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isdir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;/&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# whether a folder 
&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;templates_pool&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#get the folder into the templates_pool list
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; The existing templates pool contains these Classes: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#display the result
&lt;/span&gt;    
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;22-allocation-to-the-matched-class-folder&quot;&gt;2.2 Allocation To The Matched Class Folder&lt;/h3&gt;

&lt;p&gt;From the templates_pool above, we can check whether the Class is existing in the Bank category.&lt;/p&gt;

&lt;p&gt;Then, we can load the correspondent templateset and vectors.&lt;/p&gt;

&lt;h2 id=&quot;referrences&quot;&gt;Referrences&lt;/h2&gt;

&lt;p&gt;[1] D Cer et al. 2018  Universal Sentence Encoder&lt;/p&gt;

&lt;p&gt;[2] A Vaswani  2017  Attention Is All You Need&lt;/p&gt;

&lt;p&gt;[3] Mohit Iyyer 2015 Deep Averaging Networks&lt;/p&gt;

</description>
        <pubDate>Sat, 03 Aug 2019 00:00:00 +0800</pubDate>
        <link>https://stuartchan.github.io/2019/08/03/Week-9&10/</link>
        <guid isPermaLink="true">https://stuartchan.github.io/2019/08/03/Week-9&10/</guid>
        
        <category>GSoC2019</category>
        
        <category>NLP</category>
        
        <category>DBpedia</category>
        
        
      </item>
    
      <item>
        <title>Week 7&amp;8</title>
        <description>&lt;h1 id=&quot;the-statistics&quot;&gt;The Statistics&lt;/h1&gt;

&lt;h2 id=&quot;the-scores-for-the-answers&quot;&gt;The Scores For The Answers&lt;/h2&gt;

&lt;p&gt;This is the table for GERBIL evaluation.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Method&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;BLEU score&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Micro F1&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Micro Precision&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Micro Recall&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Macro F1&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Macro Precision&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Macro Recall&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;DBNQA&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;64.55&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.0072&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.0274&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.0041&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.0093&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.0093&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.0093&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;QALD7&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;80&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.0072&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.0282&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.0041&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.0093&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.0093&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.0093&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;dependency parsing + Spotlight&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;80&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.0037&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.0179&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.0021&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.0047&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.0047&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.0047&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;dependency parsing + Spotlight + GloVe&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;80&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.0072&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.027&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.0041&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.0093&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.0093&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.0093&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;the-scores-for-the-answer-type&quot;&gt;The Scores For The Answer Type&lt;/h2&gt;

&lt;p&gt;This table shows the statistics for answer types evaluation.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Method&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;BLEU score&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Micro F1&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Micro Precision&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Micro Recall&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Macro F1&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Macro Precision&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Macro Recall&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;DBNQA&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;64.55&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.6587&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.9167&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.514&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.5163&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.5163&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.5163&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;QALD7&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;80&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.6587&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.9167&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.514&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.5163&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.5163&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.5163&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;dependency parsing + Spotlight&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;80&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.6587&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.9167&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.514&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.5163&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.5163&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.5163&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;dependency parsing + Spotlight + GloVe&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;80&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.6587&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.9167&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.514&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.5163&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.5163&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.5163&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;

</description>
        <pubDate>Fri, 19 Jul 2019 00:00:00 +0800</pubDate>
        <link>https://stuartchan.github.io/2019/07/19/Week-7&8/</link>
        <guid isPermaLink="true">https://stuartchan.github.io/2019/07/19/Week-7&8/</guid>
        
        <category>GSoC2019</category>
        
        <category>NLP</category>
        
        <category>DBpedia</category>
        
        
      </item>
    
      <item>
        <title>Week 6</title>
        <description>&lt;h1 id=&quot;the-week--6&quot;&gt;The Week  6&lt;/h1&gt;

&lt;p&gt;This week we focus on two aspects.&lt;/p&gt;

&lt;p&gt;First is the selection of the method for calculating the semantic vectors similarity.&lt;/p&gt;

&lt;p&gt;Second is the experiment of doing benchmark evaluation in GERBIL-QA.&lt;/p&gt;

&lt;h2 id=&quot;1-the-calculation-of--vectors--similarity&quot;&gt;1. The Calculation Of  Vectors  Similarity&lt;/h2&gt;

&lt;h3 id=&quot;11-the-methods-for-measuring-the-vectors&quot;&gt;1.1 The Methods for Measuring the Vectors&lt;/h3&gt;

&lt;p&gt;Method 1: Cosine Similarity&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://res.cloudinary.com/stuarteec/image/upload/v1563698534/1342750759_1439_bzka5a.jpg&quot; alt=&quot;Cosine Similarity&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;cosine_distance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; 
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Method 2: Manhattan distance&lt;/p&gt;

&lt;p&gt;Taxi geometry or Manhattan distance or grid line distance is a vocabulary created by Herman Minkowski, which is the geometric term used in the Euclidean geometric metric space to indicate two points. The sum of the absolute wheelbases on the standard coordinate system.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;manhattan_distance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Method 3: Euclidean distance&lt;/p&gt;

&lt;p&gt;Defined on two vectors (two points): the Euclidean distance between the points is:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;euclidean_distance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;square&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Method 4: Euclidean distance standardized&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;euclidean_standardized_distance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;v1_v2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vstack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sk_v1_v2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1_v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ddof&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;zero_bit&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.000000001&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sk_v1_v2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;zero_bit&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones_like&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sk_v1_v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Method 5: Hamming Distance&lt;/p&gt;

&lt;p&gt;In information theory, the Hamming distance between two strings of equal length is the number of positions at which the corresponding symbols are different.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;hamming_distance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;^&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;bin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;mh&quot;&gt;0xffffffff&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Method 6: Chebyshev distance&lt;/p&gt;

&lt;p&gt;In mathematics, Chebyshev distance (or Tchebychev distance), maximum metric, or L∞ metric is a metric defined on a vector space where the distance between two vectors is the greatest of their differences along any coordinate dimension. It is named after Pafnuty Chebyshev.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;chebyshev_distance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Method 7: Minkowski distance&lt;/p&gt;

&lt;p&gt;The Minkowski distance is a metric in a normed vector space which can be considered as a generalization of both the Euclidean distance and the Manhattan distance.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;minkowski_distance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;square&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Method 8: Mahalanobis distance&lt;/p&gt;

&lt;p&gt;Defined on two vectors (two points), the two points are in the same distribution. The Mahalanobis distance between the points is:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;mahalanobis_distance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vstack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;XT&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# numpy.ndarray.T
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;S&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cov&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# Covariance matrix between two dimensions
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;SI&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# Inverse matrix of covariance matrix  
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;SI&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros_like&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# The Mahalanobis distance calculates the distance between two samples. There are 10 samples in total, and there are a total of 45 distances.
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;XT&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;distance_all&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;delta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;XT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;XT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;distance_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;delta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SI&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;delta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;distance_all&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distance_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distance_all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Method 9: Bray Curtis distance&lt;/p&gt;

&lt;p&gt;In ecology and biology, the Bray–Curtis dissimilarity is a statistic used to quantify the compositional dissimilarity between two different sites, based on counts at each site.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;bray_curtis_distance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;#  Biological ecological distance
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;up_v1_v2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;down_v1_v2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;zero_bit&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.000000001&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;up_v1_v2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;down_v1_v2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;zero_bit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Method 10: Pearson correlation&lt;/p&gt;

&lt;p&gt;A Pearson correlation is a number between -1 and 1 that indicates the extent to which two variables are linearly related. The Pearson correlation is also known as the “product moment correlation coefficient” (PMCC) or simply “correlation”.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;pearson_correlation_distance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;v1_v2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vstack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;corrcoef&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1_v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Method 11: Jaccard similarity coefficient&lt;/p&gt;

&lt;p&gt;The Jaccard index, also known as Intersection over Union and the Jaccard similarity coefficient (originally given the French name coefficient de communauté by Paul Jaccard), is a statistic used for gauging the similarity and diversity of sample sets. If A and B are both empty, we define J(A,B) = 1.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://res.cloudinary.com/stuarteec/image/upload/v1563698591/20180119093515865_zsct8v.png&quot; alt=&quot;Jaccar&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;jaccard_similarity_coefficient_distance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;  
    &lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;asarray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;asarray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;up&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;double&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bitwise_and&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bitwise_or&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;zero_bit&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.000000001&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;down&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;double&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bitwise_or&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;zero_bit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;jaccard&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;up&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;down&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;jaccard&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Method 12: Word Mover Distance&lt;/p&gt;

&lt;p&gt;This is the method for comparing the sentences similarity.&lt;/p&gt;

&lt;p&gt;The WMD model is based on the EMD (Earth Mover Distance) model. EMD is the same as Euclidean distance. They are definitions of distance metrics that can be used to measure the distance between two distributions. Its main application in the field of image processing and speech signal processing, WMD model is based on EMD, the scope of the model extends to the field of natural language processing. The detailed principle of EMD is not repeated here.&lt;/p&gt;

&lt;p&gt;Matt et al. associate word embedding with EMD to measure document distance. The WMD (word mover’s distance) algorithm and WCD (word centroid distance) and RWMD (relaxed word mover’s distance) algorithms are proposed.&lt;/p&gt;

&lt;p&gt;(1) The WMD algorithm uses NBOW (normalized bag-of-words, the normalized word bag model) to represent the distribution P. Where P1 represents the word itself, used to calculate the weight of the word in the current document, wherein the word i appears several times in the associated document, and the feature quantity of P1 is represented by the word vector of the word.&lt;/p&gt;

&lt;p&gt;(2) The WMD algorithm uses a Word travel cost to calculate the similarity of the word i to the word j, that is, the Euclidean distance of the word vector of the word i and the word j, and the distance value is&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;C(i,j)=|(vecI -vecJ)|. 
  Here, C(i,j) is seen as the price paid for converting the word i into the word j.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;(3) The WMD algorithm uses the Document distance to represent the distance between documents.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://res.cloudinary.com/stuarteec/image/upload/v1563698635/20171114141354849_clmc1u.jpg&quot; alt=&quot;WMD&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Define the matrix T here. Where Tij (Tij &amp;gt; = 0) indicates how much of the word i in the d document is converted into the word j in the d’ document. In order to ensure that the document d can be converted into the document d’, it is necessary to ensure that the sum of the quantities of all the words in the word d converted to d’ is di, that is, the same reason, it should also satisfy the word j converted into the d’ document in the d document. The total amount is dj, ie. We can define the distance between document d and d’ as the minimum cost of converting all words in d into words in d’.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;word_move_distance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sentence1_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sentence2_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# WORD MOVER DISTANCE, it's the important one 
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# model = gensim.models.KeyedVectors.load_word2vec_format(word2_vec_path, unicode_errors='ignore', limit=None)  # ,binary=True)
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# model.init_sims(replace=True)
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;wmdistance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sentence1_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sentence2_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;12-choose-which-method&quot;&gt;1.2 Choose Which Method:&lt;/h3&gt;

&lt;p&gt;First, we must liook at the constraints of what we want.&lt;/p&gt;

&lt;p&gt;Second, it should be for what purpose.&lt;/p&gt;

&lt;p&gt;To answer the first question, we must also keep in mind that we are using the metho on the word vectors.&lt;/p&gt;

&lt;p&gt;To the second, we should focus on the distinguishability of the semantic meanings of two phrases in the form of GloVe vectors.&lt;/p&gt;

&lt;p&gt;The Method 2(Manhattan distance) is not pertinent to our vector calculation, because we are not mearsuring a path.&lt;/p&gt;

&lt;p&gt;The Method 3(Euclidean distance) is for measuring the spatial distance between two points, which might not reflect the similarity between two words.&lt;/p&gt;

&lt;p&gt;The Method 5(Hamming Distance) is major in comparing the partial differences of two long string or documents, which might be not suitable for our phrases comparison.&lt;/p&gt;

&lt;p&gt;The Method 7(Minkowski distance) is rather closer to the combination of Method 2 and Mehod 1, but is still more pertinent to the calculation of spatial distance.&lt;/p&gt;

&lt;p&gt;Method 8(Mahalanobis distance), Method 9(Bray Curtis distance) and Method 10(Pearson correlation) are the metric based on statistic distribution, which might not be suitable for the GloVe vectors’ design ideas to represent the semantic meaning.&lt;/p&gt;

&lt;p&gt;Then, the final candidate pool leaves only the method of cosine similarity and the method of jaccard similarity coefficient distance,&lt;/p&gt;

&lt;p&gt;Let’s have the experiments:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;we randomly select a list of one hundred entities with related predicates from the generated training data, to see the precision that the two different mathod works&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Method&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Accuracy&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Cosine Similarity&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;53%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Jaccard Similarity&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;
&lt;ul&gt;
  &lt;li&gt;Analyses of The Result
Let’s take an example:
we used the natural language pair &amp;lt;”wife”, “Obama”&amp;gt; with expectation to get the entity pair &amp;lt;dbo:spouse, dbr:Barack_Obama&amp;gt;. 
So, did they succeed to get the dbo:spouse from “wife”?&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Method&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Prediction Result&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Cosine Similarity&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;dbo:spouse, dbp:parents&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Jaccard Similarity&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;dbo:speaker&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The Cosine method can catch more information in the vectors, while the Jaccard catches the distinguished 0-1 binary differences in each bit.&lt;/p&gt;

&lt;p&gt;When looking into the two lists of scores that compared the similarity between the target phrase “wife” and the list of candidate strings, we found that:
    the scores of Jaccard method are mostly greater than 0.99, or otherwise 0;
    the scores of Jaccard method have an average approximate to 0.5.&lt;/p&gt;

&lt;p&gt;And, according to &lt;a href=&quot;http://infolab.stanford.edu/~ullman/mmds/ch9.pdf&quot;&gt;a tutorial from Stanford&lt;/a&gt; [1],&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;When utilities are more detailed ratings, the Jaccard distance loses important information.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Because Jaccard is mainly used to judge the similarity between sets, he can’t reflect more information like a matrix. A simple example might be, when I make a movie recommendation, I use the data of the user to calculate the similarity between the movies. With Jaccard, I can only use a single feature of the score that a watcher feedbacked, but in Cosine’s calculation, it can add every feature of the watcher’s rating information for the movie.&lt;/p&gt;

&lt;p&gt;To sum up,&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;The Cosine similarity could be used to identify plagiarism, but will not be a good index to identify mirror sites on the internet. Whereas the Jaccard similarity will be a good index to identify mirror sites, but not so great at catching copy pasta plagiarism (within a larger document). [2]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] &lt;a href=&quot;http://infolab.stanford.edu/~ullman/&quot;&gt;Jeffrey D. Ullman&lt;/a&gt; (2017) http://infolab.stanford.edu/~ullman/mmds/ch9.pdf&lt;/p&gt;

&lt;p&gt;[2]  OluwaYetty (2018) http://techinpink.com/2017/08/04/implementing-similarity-measures-cosine-similarity-versus-jaccard-similarity/&lt;/p&gt;

</description>
        <pubDate>Sun, 07 Jul 2019 00:00:00 +0800</pubDate>
        <link>https://stuartchan.github.io/2019/07/07/Week-6/</link>
        <guid isPermaLink="true">https://stuartchan.github.io/2019/07/07/Week-6/</guid>
        
        <category>GSoC2019</category>
        
        <category>NLP</category>
        
        <category>DBpedia</category>
        
        
      </item>
    
      <item>
        <title>Week 5</title>
        <description>&lt;h1 id=&quot;the-week--5&quot;&gt;The Week  5&lt;/h1&gt;

&lt;p&gt;This week I believe it’s a diving.&lt;/p&gt;

&lt;p&gt;I can now see many things, subtle and seemingly extending to somewhere abyssal.&lt;/p&gt;

&lt;p&gt;In this week, I have tried transforming the natural language questions into SPARQL queries via the trained neural SPARQL machine models, which showed the restrictedness in the vocabulary mapping.&lt;/p&gt;

&lt;p&gt;To trace back, I noticed that the Spotlight annotation might focus on the entity recognition based on the word-wise input. Thus, the entity consisting of more than one word could have been missed.&lt;/p&gt;

&lt;p&gt;Also, to make better use of the efforts of the existing templates, it is more efficient to vectorize the new question templates generated in the previous work in order to do the comparison with the existing question templates’ embeddings.&lt;/p&gt;

&lt;p&gt;What’s more, with the helpful advice, the project is moving forward an insightful and pioneering direction.&lt;/p&gt;

&lt;h2 id=&quot;1-issues-analyses&quot;&gt;1. Issues Analyses&lt;/h2&gt;

&lt;h3 id=&quot;11-the-mismatching-while-trained-models-work-with-unprecedented-vocabulary&quot;&gt;1.1 The Mismatching While Trained Models Work With Unprecedented Vocabulary&lt;/h3&gt;

&lt;p&gt;Case 1: Entities Ambiguation&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;For example,
when the NSpM model took an input natural question that contains
&quot;region&quot;,
the model referred it to 
the &quot;dbr:wineRegion&quot; 
that it had only learned about the word similar to &quot;region&quot;,
which might be biased and divergent to the original sense.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Case 2: The Abused Mapping&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;That's to say, when the model handles a vocabulary that is not in 
the topic that it was trained, 
it would mistakenly place the unprecedented word into the place of 
another vocabulary that it had learned, then mismatching to the 
value of the replaced vocabulary.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;For example,&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;if we use the model trained on dbo:Monument templates set 
to infer the questions of the topic about location that contains the vocabulary 
that it had not learned in the training data, like:
the “rdf:type dbo:Place” and the “dbo:location” are the two of the most frequently abused in the inference
for those vocabulary unprecedented.&lt;/p&gt;

&lt;p&gt;Case 3: Issue On Reproducibility&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;The models that have attained BLEU could have not perfectly 
translated the natural language questions with a new different 
vocabulary into the correct query.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;For example,&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I picked out some of the questions in the training set as
input to the trained model to test whether it can reproduce 
the queries that are paired to these original question in the training set,
like:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“whom did xu fan marry?”,&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;the generated result was&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“select distinct var_uri where brack_open dbr_Rugrats dbo_composer var_uri”,&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;where apparently the “dbr_Rugrats” and “dbo_composer” are not related to the question,
also, the crucial entity about the person named “Xu Fan” and the entity &lt;a href=&quot;http://live.dbpedia.org/page/Xu_Fan&quot;&gt;“dbr_Xu_Fan”&lt;/a&gt; has not been properly recognized,
not matching to the correct paired query&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;“select distinct var_uri where brack_open var_uri dbo_spouse dbr_Xu_Fan sep_dot brack_close” in the training set.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Summing Up:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;The current neural SPARQL machine would be having a strong 	
dependency on the vocabulary that it have learned.
Also, the present version of the model design has mostly focused on 
the fitting of neural machine translation between the  natural 
language questions and the SPARQL queries.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;12-expanding-the-question&quot;&gt;1.2 Expanding The Question:&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;So, what should we do next?
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I got some feedback from the researches in NL2SQL that major in deploying reinforcement learning method in training the neural model to get more and more correctness in the returned answers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://res.cloudinary.com/stuarteec/image/upload/v1563698435/D-ljAtsUcAAqQ4Y_jpmsgf.jpg&quot; alt=&quot;Figure 1, SEQ2SQL(V Zhong et al.)&quot; /&gt;
Figure 1, SEQ2SQL(V Zhong et al.) [1]&lt;/p&gt;

&lt;p&gt;To elaborate on this direction of researches, the paper MAPO (Memory Augmented Policy Optimization for Program Synthesis and Semantic Parsing) [2] on NIPS 2018 is worthy of our attention.&lt;/p&gt;

&lt;p&gt;MAPO is a method based on weakly supervised and intensive learning. In the paper, it transforms the NL2SQL task into an intensive learning task based on the basic composition of the NL2SQL task and the basic elements of reinforcement learning. In MAPO, the state of reinforcement learning x is seen as the natural language problem of input and its corresponding environment (e.g. an interpreter, or an knowledge graph or, typically in the experiment in the paper, a database), and the action space A of reinforcement learning is regarded as all possible collections of programs under the current natural language problem. And each action sequence a of the enhanced learning trajectory corresponds to every possible program.&lt;/p&gt;

&lt;p&gt;It can be seen that the key to the algorithm is the training of the strategy function. In MAPO, the author uses a seq2seq model to fit the strategy function, and training for the strategy function is equivalent to training the seq2seq model.&lt;/p&gt;

&lt;p&gt;It is worth noting that in reinforcement learning, the parameter update of the strategy function is different from the deep neural network, not based on the loss function, but based on the expected return. The parameter update is in the direction of maximizing the expected return. The expected return can be given by the Reward function.&lt;/p&gt;

&lt;p&gt;Since our task is to generate program statements, we can easily run the generated program in the real environment and compare the result with the label of the weakly supervised training data to get the 0-1 binary return function. Its core idea is to correct the behavior of the agent by interacting with the environment, so as to achieve the effect of “learning”.&lt;/p&gt;

&lt;p&gt;Based on the above-mentioned intensive learning ideas, in the specific implementation, MAPO proposed the following innovative solutions. First, in order to improve the efficiency of training, the MAPO algorithm stores the sampled high-reward program into a Memory Buffer. When training the strategy function fitting network, the objective function that maximizes the expected return is divided into two parts, one is The expected return of the sampling program in the Memory Buffer, and the other part is the expected return of the sampling program outside the Buffer, as shown in the following figure.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://res.cloudinary.com/stuarteec/image/upload/v1563697922/D-luC0sVAAEQ9i__nk6wcd.png&quot; alt=&quot;MAPO,NIPS2018&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2, MAPO(NIPS 2018) [2]&lt;/p&gt;

&lt;p&gt;The team[2] applied MAPO to the &lt;a href=&quot;https://github.com/salesforce/WikiSQL&quot;&gt;WikiSQL dataset&lt;/a&gt;. For each sample on the WikiSQL data, 1000 sample programs were generated. Five high-return samples were stored in the Memory Buffer. The &lt;a href=&quot;https://nlp.stanford.edu/projects/glove/&quot;&gt;GloVe embeddings&lt;/a&gt; was applied to the strategy function fitting network. LSTM Hidden The unit is 200 and the training is 15000 steps:
&lt;img src=&quot;https://res.cloudinary.com/stuarteec/image/upload/v1563697030/MAPO.figure2_tpwugh.png&quot; alt=&quot;Figure 3, is the figure 2 from MAPO about its performances and experiments&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The learning of the connection between the the natural language question and the structured query is the major objective function, where the task is designed to let the query fit as much as possible during the machine translation training.&lt;/p&gt;

&lt;p&gt;While parallelly, the researches in NL2SQL have a comparative stronger emphasis on the result that the generated query would return and on the correctness that it would produce, because the correct return of the query is their only goal, not the matching similarity of the queries.&lt;/p&gt;

&lt;p&gt;That’s to say, their models tend to build a more direct function from the natural language question through the generation of the structured query finally towards the return answer, based on which this type of algorithms of reinforcement learning get the reward upon the right or wrong of the result of the generated query then learning to let the query generator get the more and more pertinent returned answer.&lt;/p&gt;

&lt;p&gt;To sum up, the machine translation based-model is query-driven while the reinforcement learning model that tries to take a step further can be classified as a final-answer-driven method.&lt;/p&gt;

&lt;p&gt;However, in this type of final-answer-driven models, there’s a pre-requisite that requires to embed the natural language questions, the structured query, and the answers entities into a calculable form while working with our problem.&lt;/p&gt;

&lt;h2 id=&quot;2-vectors-embedding&quot;&gt;2. Vectors Embedding&lt;/h2&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Then, why embed the data?
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The problem of the restricted limitation of the vocabulary mapping, and the abused vocabulary matching, in my humble opinion, could be traced back to the restriction of the vocabulary that it was able to learning in the training data set.&lt;/p&gt;

&lt;p&gt;So, how could we remove the limitation away from the current model?&lt;/p&gt;

&lt;p&gt;There are two aspects to be paid attention:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;1) Uniqueness and Directivity: 
	   The embedding vector must be unique in terms of 
     representing the entities, otherwise there could be conflicting mapping. And it must be 
     promising that, in the vector space, each entity-vector pair must be correctly matched in 
     one unique key-value pair.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;For example, the vocabulary “brack_close” was duplicated in the generated data file in the training data set on the topic about “place_v2”.&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;2) Comprehensive Inclusion: 
	 The vector set should, comprehensively and accurately, 
   comprise all the entities and relation properties in the DBpedia space, with inclusion in the 
   embedding of the keywords of the query grammars, e.g. “SELECT”,”DISTINCT”,”WHERE”.etc.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;After the attempts in training a new vector set on the given templates, I figured out that it might be of more efficiency employing the DBpedia embedding vector of the &lt;a href=&quot;https://github.com/dbpedia/embeddings&quot;&gt;previous projects&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] &lt;a href=&quot;https://paperswithcode.com/paper/seq2sql-generating-structured-queries-from&quot;&gt;Victor Zhong, Caiming Xiong, Richard Socher&lt;/a&gt;  (2017) SEQ2SQL: GENERATING STRUCTURED QUERIES
FROM NATURAL LANGUAGE USING REINFORCEMENT
LEARNING&lt;/p&gt;

&lt;p&gt;[2] &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Liang%2C+C&quot;&gt;Chen Liang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Norouzi%2C+M&quot;&gt;Mohammad Norouzi&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Berant%2C+J&quot;&gt;Jonathan Berant&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Le%2C+Q&quot;&gt;Quoc Le&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Lao%2C+N&quot;&gt;Ni Lao&lt;/a&gt; (2018) Memory Augmented Policy Optimization for Program Synthesis and Semantic Parsing&lt;/p&gt;

</description>
        <pubDate>Sun, 30 Jun 2019 00:00:00 +0800</pubDate>
        <link>https://stuartchan.github.io/2019/06/30/Week-5/</link>
        <guid isPermaLink="true">https://stuartchan.github.io/2019/06/30/Week-5/</guid>
        
        <category>GSoC2019</category>
        
        <category>NLP</category>
        
        <category>DBpedia</category>
        
        
      </item>
    
      <item>
        <title>Week 4</title>
        <description>&lt;h1 id=&quot;the-week--4&quot;&gt;The Week  4&lt;/h1&gt;

&lt;p&gt;Hurry up! It’s already the summer now!&lt;/p&gt;

&lt;p&gt;We should better prepare for the first evaluation in the next week!&lt;/p&gt;

&lt;p&gt;In this week, I have tried another idea to more exhaustively annotate the potential etities in the natural language.&lt;/p&gt;

&lt;p&gt;Also, I have gone through almost all the existing templates to get profound insights into the running mechanism of the current model.&lt;/p&gt;

&lt;p&gt;The connectivity in my country is intermittently irreliable that I created a new module instead of the SPARQLwrapper python libraray in order to get the results from the Virtuoso endpoint.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;
      &lt;p&gt;The constituency parsing might be more efficient for extracting the structure of a natural language sentence.&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;In the current NSpM model, there might be highly dependent on the restricted NL-SPARQL vocabulary key-value&lt;br /&gt;
  pairs to conduct the neural machne translation from the pre-processed question text to SPARQL query formatted 
  vocabulary.&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;While training the models, I noticed that if the entities annotation function that maps from the natural&lt;br /&gt;
  language text to the DBpedia entities could be added to improve the entity recognition accuracy, the performance 
  should have a better upgrade.&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;

&lt;/blockquote&gt;

&lt;h2 id=&quot;1-pre-evaluation-preparation&quot;&gt;1. Pre-evaluation Preparation&lt;/h2&gt;

&lt;h3 id=&quot;11-constituency-parsing-might-be-a-good-way-to-get-the-syntactial-structure&quot;&gt;1.1 Constituency Parsing might be a good way to get the syntactial structure&lt;/h3&gt;

&lt;p&gt;What is it? How is it diffrent from the previous method of dependency parsing?&lt;/p&gt;

&lt;p&gt;Syntactic analysis is a very important part of analyzing sentences, including constituency parsing and dependency parsing, which have very large differences.&lt;/p&gt;

&lt;p&gt;Let’s take an exemple, the constituency parse tree breaks a text into sub-phrases. Non-terminals in the tree are types of phrases, the terminals are the words in the sentence, and the edges are unlabeled. For a simple sentence “John sees Bill”, a constituency parse would be:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://res.cloudinary.com/stuarteec/image/upload/v1563699690/v2-448af772ea31d990d0f0b5414e6b1afa_hd_gmbqed.jpg&quot; alt=&quot;constituency_parse_tree&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It might do better in matching the relation between the pair(Object, Verb, Subject). However, it distract the focus from the importance of pairing the pair(Object + Property). So, I stll think dependency parsing is more pertinent to our problem in this aspect.&lt;/p&gt;

&lt;h3 id=&quot;12-the-self-made-easy-mehod-when-the-sparqlwrapper-does-not-connect&quot;&gt;1.2 The Self-made Easy Mehod when the SPARQLwrapper does not connect&lt;/h3&gt;

&lt;p&gt;The design goal is to post a request to the Virtuoso endpoint, so we simply do this:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;request&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;getVirtuoso&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                &lt;span class=&quot;s&quot;&gt;'query'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;s&quot;&gt;'default-graph-uri'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'http://dbpedia.org'&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;post&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;https://dbpedia.org/sparql&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;content&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;content&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;content&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;2-exploring-potential-issues-for-current-nspm-model&quot;&gt;2. Exploring Potential Issues for Current NSpM model&lt;/h2&gt;

&lt;h3 id=&quot;the-duplicated-issue-for-data-generation-from-templates&quot;&gt;The Duplicated Issue For Data Generation From Templates&lt;/h3&gt;

&lt;p&gt;While running the given generator.py, the terminal showed that ther was a key be added an additional value.&lt;/p&gt;

&lt;p&gt;Then I open the generated dataset file from the generator.py, searching for the redundant key-value pair. There was a vocabulary appeared twice in the dataset, which should be blamed for the duplicated mapping problem.&lt;/p&gt;

&lt;p&gt;From this, I have an idea, why not use a comprehensive set of embeddding vector set to represent the vocabulary mapping to the knowledge graph entities in DBpedia?&lt;/p&gt;

&lt;p&gt;This might help in two problems:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;1) the restrictedness of the vocabulary while the model works with the unprecedentde entity vocabulary that 
     had not learned in the traing;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;2) the global representation in the bi-LSTM encoding layer could not accurately handle the recognition of the 
     entities in natuaral language.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Sun, 23 Jun 2019 00:00:00 +0800</pubDate>
        <link>https://stuartchan.github.io/2019/06/23/Week-4/</link>
        <guid isPermaLink="true">https://stuartchan.github.io/2019/06/23/Week-4/</guid>
        
        <category>GSoC2019</category>
        
        <category>NLP</category>
        
        <category>DBpedia</category>
        
        
      </item>
    
      <item>
        <title>Week 3</title>
        <description>&lt;h1 id=&quot;the-week--3&quot;&gt;The Week  3&lt;/h1&gt;

&lt;p&gt;This week, I have replaced the exited tools with my own innovative and independent modules pertinent to DBpedia. However, after doing so, the problem emerges while doing the BLEU evaluation.&lt;/p&gt;

&lt;p&gt;I have buried myself into coding and checking on this weekends, while spending the whole weekend in evaluating and considering the flexibility of the generation component.&lt;/p&gt;

&lt;p&gt;At present, the component only accepts the limited questions matched to certain regexes and generate the template queries that do not contain explicit URLs.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;
      &lt;p&gt;the current component rejects the questions’ formats in  annotations_monument.csv&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;the function is so restricted that, the question which it had not accepted would be accepted well after
  fine-tuning some uppper/lower class or even arranging the order:
   1) the syntactic parsing dependency is not working very well in transforming the similar question in the
      templates; 
   2) the regex matching stays fragile, not robust enough to catch the syntactical structure of a new question; 
   3) while doing the BLEU, I replace the variables in the templates with exact vocabularies, and the module
      could not handle it and refused to generate the template queries, which tells me that it is far from 
      perfect.&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;

&lt;/blockquote&gt;

&lt;p&gt;The work in this week’s goals:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;
      &lt;p&gt;Improving the Generation Component&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;Thinking About The Evaluations&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;1-improving-the-generation-component&quot;&gt;1. Improving the Generation Component&lt;/h2&gt;

&lt;h3 id=&quot;thinking-deeper-to-find-a-method-to-more-intelligently-represent-the-syntactic-structures-of-the-questions&quot;&gt;Thinking deeper to find a method to more intelligently represent the syntactic structures of the questions.&lt;/h3&gt;

&lt;p&gt;For two reasons:&lt;/p&gt;
&lt;blockquote&gt;

  &lt;p&gt;1) It helps to improve the automation of templates generation, especially in query templates generation, for 
   which I have worked on three paths:
      Syntactic parsing with entity detection to seize the structure, that?s the currently primary option but not
      highly automatic;
      Use sequence to sequence LSTM to train a model for natural language to query, which is estimated to be quite &amp;gt;       time-consuming;
      Deploy the Universal Sentence Encoder to embed the syntactic structures into embedding vectors as the 
      representation, which would also help a lot in the next phase of project while this requires some endeavors.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;2) If the representation of the syntactic structures is efficient enough, it will accelerate the computation 
  for matching the templates in the Templates Bank.  It reinforces the grounding of Templates Comparison, which is 
  to be one of the goals in next week too.
  And from this, it derives two aspects of further consideration:
  the storing of  the templates and their representation, whether to store in the database or, maybe more 
  efficient for computing the matching, in Hadoop/Spark via the python API;
  Still, the representation of the templates or their syntactic structures, for easier matching, like calculating 
  the similarity of two embedding vectors to get the most matched.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;2-thinking-about-the-evaluations&quot;&gt;2. Thinking About The Evaluations&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;1) After two days’ trial on evaluating the performance, the lacking in robustness is apparent after the 
      changing in this week.
      So, how to improve the model based on the experiments that we had.
      First, the dependency parsing might be a good idea.
      Second, the regexes should be more intelligent.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;2) We will try on the GERBIL plateform, for its convienience.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;the-commit-of-this-week&quot;&gt;The Commit of This Week&lt;/h2&gt;

&lt;p&gt;This is the &lt;a href=&quot;https://github.com/dbpedia/neural-qa/compare/master...StuartCHAN:gsoc-stuart?expand=1&quot;&gt;summing-up&lt;/a&gt; the works that I have done.&lt;/p&gt;

&lt;p&gt;For the next week, I will do more works concerning about the dependency parsing which pertains to the automated query template generation with robustness.&lt;/p&gt;
</description>
        <pubDate>Sun, 16 Jun 2019 00:00:00 +0800</pubDate>
        <link>https://stuartchan.github.io/2019/06/16/Week-3/</link>
        <guid isPermaLink="true">https://stuartchan.github.io/2019/06/16/Week-3/</guid>
        
        <category>GSoC2019</category>
        
        <category>NLP</category>
        
        <category>DBpedia</category>
        
        
      </item>
    
      <item>
        <title>Week 2</title>
        <description>&lt;h1 id=&quot;the-week--2&quot;&gt;The Week  2&lt;/h1&gt;

&lt;p&gt;Let’s have a look into this week’s goals:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;
      &lt;p&gt;Entity Recognition&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;Template Generation&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;1-entity-recognition&quot;&gt;1. Entity Recognition&lt;/h2&gt;

&lt;h3 id=&quot;strategy-i&quot;&gt;Strategy I&lt;/h3&gt;
&lt;p&gt;I had planned to deploy the pyspotlight for detecting the DBpedia entities in the question.
However, it seems the connectivity to the server is intermittently unstable, which might result in renting a remote server or it might stumble even after the doing so.
So I jumped to the Strategy 2.&lt;/p&gt;

&lt;h3 id=&quot;strategy-ii&quot;&gt;Strategy II&lt;/h3&gt;
&lt;p&gt;I rent a server as the transition server.
Then we can freely make the request to the Spotlight:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;annotate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;headers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;'Accept'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'application/json'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'confidence'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'0.35'&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;post&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'http://api.dbpedia-spotlight.org/en/annotate'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;headers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;headers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;response_json&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Now it's so easy！
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;2-template-generation&quot;&gt;2. Template Generation&lt;/h2&gt;

&lt;p&gt;This function depends on the semantic parsing of the question to get the core synthetic structure of the question in order to build a template.&lt;/p&gt;

&lt;p&gt;When thinking of Audrey Hepburn, we might ask:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&quot;Which movie does Audrey Hepburn star ?&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;21-semantic-parsing&quot;&gt;2.1 Semantic Parsing&lt;/h3&gt;
&lt;p&gt;The question should be parsed to get the synthetic structure by:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'Which'&lt;/span&gt;, &lt;span class=&quot;s1&quot;&gt;'JJ'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;,
 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'movie'&lt;/span&gt;, &lt;span class=&quot;s1&quot;&gt;'NN'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;,
 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'does'&lt;/span&gt;, &lt;span class=&quot;s1&quot;&gt;'VBZ'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;,
 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'Audrey'&lt;/span&gt;, &lt;span class=&quot;s1&quot;&gt;'NNP'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;,
 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'star'&lt;/span&gt;, &lt;span class=&quot;s1&quot;&gt;'VB'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;,
 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'?'&lt;/span&gt;, &lt;span class=&quot;s1&quot;&gt;'.'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;then we get the semantic tree like:
&lt;img src=&quot;https://res.cloudinary.com/stuarteec/image/upload/v1563696970/Audrey_tree1_rlocap.png&quot; alt=&quot;Audrey_tree1&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;err&quot;&gt;('Which&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;movie&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;does&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&amp;lt;A&amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;star&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;?'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Person'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;Problem that I think for I :
		The semantic parsing is not automated enough that it needs further improvement.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;21-query-template&quot;&gt;2.1 Query Template&lt;/h3&gt;
&lt;p&gt;Then, based on the work in the last step, we construct the semantic structure for the query template:
&lt;img src=&quot;https://res.cloudinary.com/stuarteec/image/upload/v1563696970/Audrey_tree2_gq0wof.png&quot; alt=&quot;Audrey_tree2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And then, with the help of matching the regex of the question,&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Movie&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Particle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;regex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Question&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Pos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;DT&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nouns&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;interpret&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;match&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;match&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;words&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;IsMovie&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HasName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Actor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Particle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;regex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nouns&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;interpret&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;match&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;match&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;words&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;IsPerson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HasKeyword&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Director&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Particle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;regex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nouns&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;interpret&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;match&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;match&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;words&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;IsPerson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HasKeyword&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ListMoviesQuestion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;QuestionTemplate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;regex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Lemma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;list&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Lemma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;movie&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Lemma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;film&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;interpret&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;match&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;movie&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;IsMovie&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NameOf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;movie&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;enum&quot;&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;we can get the query output:&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;SELECT DISTINCT ?a WHERE &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
   ?a rdf:type dbpedia-owl:Film.
   ?x0 dbpprop:starring ?b.
   ?a foaf:name ?x2.
   ?b rdf:type &amp;lt;A&amp;gt;
   ?b rdfs:label &lt;span class=&quot;s2&quot;&gt;&quot;Audrey
   ?b rdfs:label &quot;&lt;/span&gt;Audrey
   &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;Problem that I think for II:
		We can see, the regex needs to be built from the parsing result.
		So, when dealing with a totally new question, it risks of disability to immediately figure 
		out the new regex for the new questions that the model has never faced before.
		How? With what? Based on what?
		I must go deeper.
		The regex is still not immediately created while facing a new genre of  question, this 
		needs  to be focused.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;summing-up-for-week-2&quot;&gt;Summing Up For Week 2&lt;/h2&gt;

&lt;p&gt;The Templates Generator component is basically built, 
however, still far from perfect.
The next days should delve into the automation about creating the new regexes for the never-processed genres of questions, for which I consider the semantic parsing is the point to break through.&lt;/p&gt;

&lt;p&gt;We have certain achievement this week, but still a long way to go.&lt;/p&gt;

</description>
        <pubDate>Sat, 08 Jun 2019 00:00:00 +0800</pubDate>
        <link>https://stuartchan.github.io/2019/06/08/Week-2/</link>
        <guid isPermaLink="true">https://stuartchan.github.io/2019/06/08/Week-2/</guid>
        
        <category>GSoC2019</category>
        
        <category>NLP</category>
        
        <category>DBpedia</category>
        
        
      </item>
    
      <item>
        <title>Architecture</title>
        <description>&lt;h1 id=&quot;the-pipeline-of-architecture&quot;&gt;The Pipeline of Architecture&lt;/h1&gt;

&lt;p&gt;The model architecture is like:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://res.cloudinary.com/stuarteec/image/upload/v1566790535/Atten_NSPM00.v02_y762nv.png&quot; alt=&quot;architecture and pipeline&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;pipeline&quot;&gt;Pipeline&lt;/h1&gt;

&lt;h2 id=&quot;1-question-generation-from-natural-language&quot;&gt;1. Question Generation from Natural Language&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://res.cloudinary.com/stuarteec/image/upload/v1566791144/Atten_NSPM00.v02_2_ep2wsi.png&quot; alt=&quot;QuestionsGeneration&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To generate the templates from the text extracted from Wikipedia, we need first to get the sentences containing RDFs.&lt;/p&gt;

&lt;h3 id=&quot;11-input-from-natural-language&quot;&gt;1.1 Input from Natural Language&lt;/h3&gt;

&lt;p&gt;We use the natural language passage from Wikipedia as input :&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    Audrey Hepburn was a British actress and humanitarian. Recognised as a movie star and fashion icon, Hepburn was active during Hollywood's Golden Age. She was ranked by the American Film Institute as the third-greatest female screen legend in Golden Age Hollywood, and was inducted into the International Best Dressed List Hall of Fame. She rose to star in Roman Holiday in 1953.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;https://res.cloudinary.com/stuarteec/image/upload/v1563699161/v2-6a37b3b2f4db3949137d90642df08ff4_hd_uu57j6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;12-pre-processing--rdf-filtering-with-paraphrases&quot;&gt;1.2 Pre-processing &amp;amp; RDF filtering with paraphrases&lt;/h3&gt;

&lt;p&gt;The passage goes through the &lt;code class=&quot;highlighter-rouge&quot;&gt;pre-processing&lt;/code&gt; to remove the redundant characters and punctuations.&lt;/p&gt;

&lt;p&gt;The cleaned passage will go into the part of neural coreferrences resolutions by spaCy and neuralcoref to confirm the subject’s words of the topic, like:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Barack Obama&lt;/code&gt;–&amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;he&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;him&lt;/code&gt; ;&lt;/p&gt;

&lt;p&gt;then, we use the wordnet via nltk to paraphrase the verbal forms of the predicate word,&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;dbo:parent&lt;/code&gt; –lemmatize–&amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;parent&lt;/code&gt; –paraphrase–&amp;gt; [ &lt;code class=&quot;highlighter-rouge&quot;&gt;father&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;mother&lt;/code&gt;, … ]&lt;/p&gt;

&lt;p&gt;which can facilate the pinpointing of the RDF and filter out those sentences that can match the &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt; coreference(subject), paraphrase(predicate), object &amp;gt;&lt;/code&gt; triple.&lt;/p&gt;

&lt;h3 id=&quot;13-output-the-question&quot;&gt;1.3 Output The Question&lt;/h3&gt;

&lt;p&gt;Then we get the output question, for example:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;s2&quot;&gt;&quot; The &amp;lt;father&amp;gt; of &amp;lt;Barack&amp;gt; is &amp;lt;Obama Sr&amp;gt;. &quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;after which, we use the DBpedia-Spotlight API to detect the category of the entity, whether to classify it by using which interogative word to ask the question, e.g.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Obama Sr&lt;/code&gt; –spotlight–&amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;dbo:Person&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;then we can pick the &lt;code class=&quot;highlighter-rouge&quot;&gt;who&lt;/code&gt; from the list of interogative words [&lt;code class=&quot;highlighter-rouge&quot;&gt;who&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;what&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;where&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;which&lt;/code&gt;] for questioning.&lt;/p&gt;

&lt;p&gt;The output question will be converted to like:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;s2&quot;&gt;&quot; who is the father of &amp;lt;dbr:Barack_Obama&amp;gt; ? &quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;which will be: &lt;code class=&quot;highlighter-rouge&quot;&gt;dbo:Person;;; who is the father of &amp;lt;A&amp;gt; ;&lt;/code&gt; with the annotation of Spotlight.&lt;/p&gt;

&lt;h2 id=&quot;2-matching-the-new-template-question-with-existing-templateset-to-see-whether-theres-already-a-similar-template-for-this-question&quot;&gt;2. Matching the new template question with existing templateset to see whether there’s already a similar template for this question&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://res.cloudinary.com/stuarteec/image/upload/v1566791144/Atten_NSPM00.v02_4_f7jrnt.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With the help of the product of Universal Sentence Encoder by importing TensorFlow-hub, the model calculate the vector similarity between the existing templates and these new questions.&lt;/p&gt;

&lt;p&gt;If the similarity score can pass the treshold, the system automatically fetch the matched item’s template query to concatenate into the new question;&lt;/p&gt;

&lt;p&gt;else if the treshold doesn’t pass that there is no similar question for this new question, the system will also generate the query based on the annotation triple and the regex to buid a template query for it.&lt;/p&gt;

&lt;h2 id=&quot;4-transformer-with-entities-annoatated&quot;&gt;4. Transformer with Entities Annoatated&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://res.cloudinary.com/stuarteec/image/upload/v1566816472/atten_figure1_mrubms.png&quot; alt=&quot;Attention is all we need&quot; title=&quot;Attention Is All You Need .Figure 1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For the prominent performance in neural machine translation task of Transformer with attention mechanism, it can be one state-of-the-art neural model to do the natural language to SPARQL task.&lt;/p&gt;

&lt;p&gt;First, we have a look at the training data, which consist of two parts, namely, &lt;code class=&quot;highlighter-rouge&quot;&gt;data.en&lt;/code&gt; the source data where there’re the natural language questions with entities annoated, and &lt;code class=&quot;highlighter-rouge&quot;&gt;data.sparql&lt;/code&gt; the target data where there’re the correspondent SPARQL queries.&lt;/p&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;In data.en:
    what is the total population of dbr_Barranca_de_Otates?
    who painted the dbr_Jekyll_+_Hyde?
    what is the total population of dbr_Lemithou?
    when did dbr_Haunting_of_Cassie_Palmer creator die?
    ...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;In data.sparql:
    select var_uri where brack_open dbr_Barranca_de_Otates dbo_populationTotal var_uri sep_dot brack_close
    select distinct var_uri where brack_open dbr_Jekyll_+_Hyde dbp_artist var_uri sep_dot brack_close
    select var_uri where brack_open dbr_Lemithou dbo_populationTotal var_uri sep_dot brack_close
    select distinct var_date where brack_open dbr_Haunting_of_Cassie_Palmer dbo_creator var_x sep_dot var_x dbo_deathDate var_date sep_dot brack_close
    ...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

</description>
        <pubDate>Sun, 02 Jun 2019 00:00:00 +0800</pubDate>
        <link>https://stuartchan.github.io/2019/06/02/Architecture/</link>
        <guid isPermaLink="true">https://stuartchan.github.io/2019/06/02/Architecture/</guid>
        
        <category>GSoC2019</category>
        
        <category>NLP</category>
        
        <category>DBpedia</category>
        
        
      </item>
    
  </channel>
</rss>
