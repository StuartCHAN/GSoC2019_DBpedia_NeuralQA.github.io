<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Stuart Chen's Blog</title>
    <description> L'air immense ouvre et referme mon livre, la vague en poudre ose jaillir des rocs! </description>
    <link>https://stuartchan.github.io/</link>
    <atom:link href="https://stuartchan.github.io/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Mon, 01 Jul 2019 20:44:10 +0800</pubDate>
    <lastBuildDate>Mon, 01 Jul 2019 20:44:10 +0800</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>Week 5</title>
        <description>&lt;h1 id=&quot;the-week--5&quot;&gt;The Week  5&lt;/h1&gt;

&lt;p&gt;This week I believe it’s a diving.&lt;/p&gt;

&lt;p&gt;I can now see many things, subtle and seemingly extending to somewhere abyssal.&lt;/p&gt;

&lt;p&gt;In this week, I have tried transforming the natural language questions into SPARQL queries via the trained neural SPARQL machine models, which showed the restrictedness in the vocabulary mapping.&lt;/p&gt;

&lt;p&gt;To trace back, I noticed that the Spotlight annotation might focus on the entity recognition based on the word-wise input. Thus, the entity consisting of more than one word could have been missed.&lt;/p&gt;

&lt;p&gt;Also, to make better use of the efforts of the existing templates, it is more efficient to vectorize the new question templates generated in the previous work in order to do the comparison with the existing question templates’ embeddings.&lt;/p&gt;

&lt;p&gt;What’s more, with the helpful advice, the project is moving forward an insightful and pioneering direction.&lt;/p&gt;

&lt;h2 id=&quot;1-issues-analyses&quot;&gt;1. Issues Analyses&lt;/h2&gt;

&lt;h3 id=&quot;11-the-mismatching-while-trained-models-work-with-unprecedented-vocabulary&quot;&gt;1.1 The Mismatching While Trained Models Work With Unprecedented Vocabulary&lt;/h3&gt;

&lt;p&gt;Case 1: Entities Ambiguation&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;For example,
when the NSpM model took an input natural question that contains
&quot;region&quot;,
the model referred it to the &quot;dbr:wineRegion&quot; that it had only
learned,
which might be biased and divergent to the original sense.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Case 2: The Abused Mapping&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;That's to say, when the model handles a vocabulary that is not in 
the topic that it was trained, 
it would mistakenly place the unprecedented word into the place of 
another vocabulary that it had learned, then mismatching to the 
value of the replaced vocabulary.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Case 3: Issue On Reproducibility&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;The models that have attained BLEU could have not perfectly 
translated the natural language questions with a new different 
vocabulary into the correct query.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Summing Up:
	The current neural SPARQL machine would be having a strong 	
	dependency on the vocabulary that it have learned.
	Also, the present version of the model design has mostly focused on 
	the fitting of neural machine translation between the  natural 
	language questions and the SPARQL queries.&lt;/p&gt;

&lt;h3 id=&quot;12-expanding-the-question&quot;&gt;1.2 Expanding The Question:&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;So, what should we do next?
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I got some feedback from the researches in NL2SQL that major in deploying reinforcement learning method in training the neural model to get more and more correctness in the returned answers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic2.zhimg.com/80/v2-c45ddc6df9f5165326d871dcf7f31959_hd.jpg&quot; alt=&quot;Figure 1, SEQ2SQL(V Zhong et al.)&quot; /&gt;&lt;/p&gt;

&lt;p&gt;​																		Figure 1, SEQ2SQL(V Zhong et al.)&lt;/p&gt;

&lt;p&gt;The learning of the connection between the the natural language question and the structured query is the major objective function, where the  task is designed to let the query fit as much as possible during the machine translation training.&lt;/p&gt;

&lt;p&gt;While parallelly, the researches in NL2SQL have a comparative stronger emphasis on the result that the generated query would return and on the correctness that it would produce, because the correct return of the query is their only goal, not the matching similarity of the queries.&lt;/p&gt;

&lt;p&gt;That’s to say, their models tend to build a more direct function from the natural language question through the generation of the structured query finally towards the return answer, based on which this type of algorithms of reinforcement learning get the reward upon the right or wrong of the result of the generated query then learning to let the query generator get the more and more pertinent returned answer.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;To sum up, the machine translation based-model is query-driven while 	 the reinforcement learning model that tries to take a step further 	can be classified as a final-answer-driven method.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;However, in this type of final-answer-driven models, there’s a pre-requisite that requires to embed the natural language questions, the structured query, and the answers entities into a calculable form while working with our problem.&lt;/p&gt;

&lt;h2 id=&quot;2-vectors-embedding&quot;&gt;2. Vectors Embedding&lt;/h2&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Then, why embed the data?
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The problem of the restricted limitation of the vocabulary mapping, and the abused vocabulary matching, in my humble opinion, could be traced back to the restriction of the vocabulary that it was able to learning in the training data set.&lt;/p&gt;

&lt;p&gt;So, how could we remove the limitation away from the current model?&lt;/p&gt;

&lt;p&gt;There are two aspects to be paid attention:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;1) Uniqueness and Directivity: The embedding vector must be unique in terms of 
     representing the entities, otherwise there could be conflicting mapping. And it must be 
     promising that, in the vector space, each entity-vector pair must be correctly matched in 
     one unique key-value pair.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;2) Comprehensive Inclusion: The vector set should, comprehensively and accurately, 
   comprise all the entities and relation properties in the DBpedia space, with inclusion in the 
   embedding of the keywords of the query grammars, e.g. “SELECT”,”DISTINCT”,”WHERE”.etc.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;After the attempts in training a new vector set on the given templates, I figured out that it might be of more efficiency employing the DBpedia embedding vector of the &lt;a href=&quot;https://github.com/dbpedia/embeddings&quot;&gt;previous projects&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] &lt;a href=&quot;https://paperswithcode.com/paper/seq2sql-generating-structured-queries-from&quot;&gt;Victor Zhong, Caiming Xiong, Richard Socher&lt;/a&gt;  (2017) SEQ2SQL: GENERATING STRUCTURED QUERIES
FROM NATURAL LANGUAGE USING REINFORCEMENT
LEARNING&lt;/p&gt;

&lt;p&gt;[2] &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Liang%2C+C&quot;&gt;Chen Liang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Norouzi%2C+M&quot;&gt;Mohammad Norouzi&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Berant%2C+J&quot;&gt;Jonathan Berant&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Le%2C+Q&quot;&gt;Quoc Le&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Lao%2C+N&quot;&gt;Ni Lao&lt;/a&gt; (2018) Memory Augmented Policy Optimization for Program Synthesis and Semantic Parsing&lt;/p&gt;
</description>
        <pubDate>Sun, 30 Jun 2019 00:00:00 +0800</pubDate>
        <link>https://stuartchan.github.io/2019/06/30/Week-5/</link>
        <guid isPermaLink="true">https://stuartchan.github.io/2019/06/30/Week-5/</guid>
        
        <category>GSoC2019</category>
        
        <category>NLP</category>
        
        <category>DBpedia</category>
        
        
      </item>
    
      <item>
        <title>Week 4</title>
        <description>&lt;h1 id=&quot;the-week--4&quot;&gt;The Week  4&lt;/h1&gt;

&lt;p&gt;Hurry up! It’s already the summer now!&lt;/p&gt;

&lt;p&gt;We should better prepare for the first evaluation in the next week!&lt;/p&gt;

&lt;p&gt;In this week, I have tried another idea to more exhaustively annotate the potential etities in the natural language.&lt;/p&gt;

&lt;p&gt;Also, I have gone through almost all the existing templates to get profound insights into the running mechanism of the current model.&lt;/p&gt;

&lt;p&gt;The connectivity in my country is intermittently irreliable that I created a new module instead of the SPARQLwrapper python libraray in order to get the results from the Virtuoso endpoint.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;
      &lt;p&gt;The constituency parsing might be more efficient for extracting the structure of a natural language sentence.&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;In the current NSpM model, there might be highly dependent on the restricted NL-SPARQL vocabulary key-value&lt;br /&gt;
  pairs to conduct the neural machne translation from the pre-processed question text to SPARQL query formatted 
  vocabulary.&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;While training the models, I noticed that if the entities annotation function that maps from the natural&lt;br /&gt;
  language text to the DBpedia entities could be added to improve the entity recognition accuracy, the performance 
  should have a better upgrade.&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;

&lt;/blockquote&gt;

&lt;h2 id=&quot;1-pre-evaluation-preparation&quot;&gt;1. Pre-evaluation Preparation&lt;/h2&gt;

&lt;h3 id=&quot;11-constituency-parsing-might-be-a-good-way-to-get-the-syntactial-structure&quot;&gt;1.1 Constituency Parsing might be a good way to get the syntactial structure&lt;/h3&gt;

&lt;p&gt;What is it? How is it diffrent from the previous method of dependency parsing?&lt;/p&gt;

&lt;p&gt;Syntactic analysis is a very important part of analyzing sentences, including constituency parsing and dependency parsing, which have very large differences.&lt;/p&gt;

&lt;p&gt;Let’s take an exemple, the constituency parse tree breaks a text into sub-phrases. Non-terminals in the tree are types of phrases, the terminals are the words in the sentence, and the edges are unlabeled. For a simple sentence “John sees Bill”, a constituency parse would be:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic3.zhimg.com/80/v2-448af772ea31d990d0f0b5414e6b1afa_hd.jpg&quot; alt=&quot;constituency_parse_tree&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It might do better in matching the relation between the pair(Object, Verb, Subject). However, it distract the focus from the importance of pairing the pair(Object + Property). So, I stll think dependency parsing is more pertinent to our problem in this aspect.&lt;/p&gt;

&lt;h3 id=&quot;12-the-self-made-easy-mehod-when-the-sparqlwrapper-does-not-connect&quot;&gt;1.2 The Self-made Easy Mehod when the SPARQLwrapper does not connect&lt;/h3&gt;

&lt;p&gt;The design goal is to post a request to the Virtuoso endpoint, so we simply do this:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;request&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;getVirtuoso&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                &lt;span class=&quot;s&quot;&gt;'query'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;s&quot;&gt;'default-graph-uri'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'http://dbpedia.org'&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;post&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;https://dbpedia.org/sparql&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;content&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;content&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;content&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;2-exploring-potential-issues-for-current-nspm-model&quot;&gt;2. Exploring Potential Issues for Current NSpM model&lt;/h2&gt;

&lt;h3 id=&quot;the-duplicated-issue-for-data-generation-from-templates&quot;&gt;The Duplicated Issue For Data Generation From Templates&lt;/h3&gt;

&lt;p&gt;While running the given generator.py, the terminal showed that ther was a key be added an additional value.&lt;/p&gt;

&lt;p&gt;Then I open the generated dataset file from the generator.py, searching for the redundant key-value pair. There was a vocabulary appeared twice in the dataset, which should be blamed for the duplicated mapping problem.&lt;/p&gt;

&lt;p&gt;From this, I have an idea, why not use a comprehensive set of embeddding vector set to represent the vocabulary mapping to the knowledge graph entities in DBpedia?&lt;/p&gt;

&lt;p&gt;This might help in two problems:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;1) the restrictedness of the vocabulary while the model works with the unprecedentde entity vocabulary that 
     had not learned in the traing;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;2) the global representation in the bi-LSTM encoding layer could not accurately handle the recognition of the &amp;gt;      entities in natuaral language.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Sun, 23 Jun 2019 00:00:00 +0800</pubDate>
        <link>https://stuartchan.github.io/2019/06/23/Week-4/</link>
        <guid isPermaLink="true">https://stuartchan.github.io/2019/06/23/Week-4/</guid>
        
        <category>GSoC2019</category>
        
        <category>NLP</category>
        
        <category>DBpedia</category>
        
        
      </item>
    
      <item>
        <title>Week 3</title>
        <description>&lt;h1 id=&quot;the-week--3&quot;&gt;The Week  3&lt;/h1&gt;

&lt;p&gt;This week, I have replaced the exited tools with my own innovative and independent modules pertinent to DBpedia. However, after doing so, the problem emerges while doing the BLEU evaluation.&lt;/p&gt;

&lt;p&gt;I have buried myself into coding and checking on this weekends, while spending the whole weekend in evaluating and considering the flexibility of the generation component.&lt;/p&gt;

&lt;p&gt;At present, the component only accepts the limited questions matched to certain regexes and generate the template queries that do not contain explicit URLs.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;
      &lt;p&gt;the current component rejects the questions’ formats in  annotations_monument.csv&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;the function is so restricted that, the question which it had not accepted would be accepted well after
  fine-tuning some uppper/lower class or even arranging the order:
   1) the syntactic parsing dependency is not working very well in transforming the similar question in the
      templates; 
   2) the regex matching stays fragile, not robust enough to catch the syntactical structure of a new question; 
   3) while doing the BLEU, I replace the variables in the templates with exact vocabularies, and the module
      could not handle it and refused to generate the template queries, which tells me that it is far from 
      perfect.&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;

&lt;/blockquote&gt;

&lt;p&gt;The work in this week’s goals:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;
      &lt;p&gt;Improving the Generation Component&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;Thinking About The Evaluations&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;1-improving-the-generation-component&quot;&gt;1. Improving the Generation Component&lt;/h2&gt;

&lt;h3 id=&quot;thinking-deeper-to-find-a-method-to-more-intelligently-represent-the-syntactic-structures-of-the-questions&quot;&gt;Thinking deeper to find a method to more intelligently represent the syntactic structures of the questions.&lt;/h3&gt;

&lt;p&gt;For two reasons:&lt;/p&gt;
&lt;blockquote&gt;

  &lt;p&gt;1) It helps to improve the automation of templates generation, especially in query templates generation, for 
   which I have worked on three paths:
      Syntactic parsing with entity detection to seize the structure, that?s the currently primary option but not
      highly automatic;
      Use sequence to sequence LSTM to train a model for natural language to query, which is estimated to be quite &amp;gt;       time-consuming;
      Deploy the Universal Sentence Encoder to embed the syntactic structures into embedding vectors as the 
      representation, which would also help a lot in the next phase of project while this requires some endeavors.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;2) If the representation of the syntactic structures is efficient enough, it will accelerate the computation 
  for matching the templates in the Templates Bank.  It reinforces the grounding of Templates Comparison, which is 
  to be one of the goals in next week too.
  And from this, it derives two aspects of further consideration:
  the storing of  the templates and their representation, whether to store in the database or, maybe more 
  efficient for computing the matching, in Hadoop/Spark via the python API;
  Still, the representation of the templates or their syntactic structures, for easier matching, like calculating 
  the similarity of two embedding vectors to get the most matched.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;2-thinking-about-the-evaluations&quot;&gt;2. Thinking About The Evaluations&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;1) After two days’ trial on evaluating the performance, the lacking in robustness is apparent after the 
      changing in this week.
      So, how to improve the model based on the experiments that we had.
      First, the dependency parsing might be a good idea.
      Second, the regexes should be more intelligent.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;2) We will try on the GERBIL plateform, for its convienience.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;the-commit-of-this-week&quot;&gt;The Commit of This Week&lt;/h2&gt;

&lt;p&gt;This is the &lt;a href=&quot;https://github.com/dbpedia/neural-qa/compare/master...StuartCHAN:gsoc-stuart?expand=1&quot;&gt;summing-up&lt;/a&gt; the works that I have done.&lt;/p&gt;

&lt;p&gt;For the next week, I will do more works concerning about the dependency parsing which pertains to the automated query template generation with robustness.&lt;/p&gt;
</description>
        <pubDate>Sun, 16 Jun 2019 00:00:00 +0800</pubDate>
        <link>https://stuartchan.github.io/2019/06/16/Week-3/</link>
        <guid isPermaLink="true">https://stuartchan.github.io/2019/06/16/Week-3/</guid>
        
        <category>GSoC2019</category>
        
        <category>NLP</category>
        
        <category>DBpedia</category>
        
        
      </item>
    
      <item>
        <title>Week 2</title>
        <description>&lt;h1 id=&quot;the-week--2&quot;&gt;The Week  2&lt;/h1&gt;

&lt;p&gt;Let’s have a look into this week’s goals:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;
      &lt;p&gt;Entity Recognition&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;Template Generation&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;1-entity-recognition&quot;&gt;1. Entity Recognition&lt;/h2&gt;

&lt;h3 id=&quot;strategy-i&quot;&gt;Strategy I&lt;/h3&gt;
&lt;p&gt;I had planned to deploy the pyspotlight for detecting the DBpedia entities in the question.
However, it seems the connectivity to the server is intermittently unstable, which might result in renting a remote server or it might stumble even after the doing so.
So I jumped to the Strategy 2.&lt;/p&gt;

&lt;h3 id=&quot;strategy-ii&quot;&gt;Strategy II&lt;/h3&gt;
&lt;p&gt;I rent a server as the transition server.
Then we can freely make the request to the Spotlight:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;annotate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;headers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;'Accept'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'application/json'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'confidence'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'0.35'&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;post&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'http://api.dbpedia-spotlight.org/en/annotate'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;headers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;headers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;response_json&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Now it's so easy！
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;2-template-generation&quot;&gt;2. Template Generation&lt;/h2&gt;

&lt;p&gt;This function depends on the semantic parsing of the question to get the core synthetic structure of the question in order to build a template.&lt;/p&gt;

&lt;p&gt;When thinking of Audrey Hepburn, we might ask:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&quot;Which movie does Audrey Hepburn star ?&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;21-semantic-parsing&quot;&gt;2.1 Semantic Parsing&lt;/h3&gt;
&lt;p&gt;The question should be parsed to get the synthetic structure by:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'Which'&lt;/span&gt;, &lt;span class=&quot;s1&quot;&gt;'JJ'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;,
 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'movie'&lt;/span&gt;, &lt;span class=&quot;s1&quot;&gt;'NN'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;,
 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'does'&lt;/span&gt;, &lt;span class=&quot;s1&quot;&gt;'VBZ'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;,
 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'Audrey'&lt;/span&gt;, &lt;span class=&quot;s1&quot;&gt;'NNP'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;,
 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'star'&lt;/span&gt;, &lt;span class=&quot;s1&quot;&gt;'VB'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;,
 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'?'&lt;/span&gt;, &lt;span class=&quot;s1&quot;&gt;'.'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;then we get the semantic tree like:
&lt;img src=&quot;https://pic2.zhimg.com/80/v2-56675bcbf18f06a3fd2ee116c87a5381_hd.png&quot; alt=&quot;Audrey_tree1&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;err&quot;&gt;('Which&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;movie&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;does&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&amp;lt;A&amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;star&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;?'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Person'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;Problem that I think for I :
		The semantic parsing is not automated enough that it needs further improvement.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;21-query-template&quot;&gt;2.1 Query Template&lt;/h3&gt;
&lt;p&gt;Then, based on the work in the last step, we construct the semantic structure for the query template:
&lt;img src=&quot;https://pic1.zhimg.com/80/v2-740e8b38b75543e8584305caeca2cb38_hd.jpg&quot; alt=&quot;Audrey_tree2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And then, with the help of matching the regex of the question,&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Movie&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Particle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;regex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Question&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Pos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;DT&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nouns&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;interpret&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;match&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;match&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;words&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;IsMovie&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HasName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Actor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Particle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;regex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nouns&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;interpret&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;match&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;match&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;words&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;IsPerson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HasKeyword&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Director&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Particle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;regex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nouns&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;interpret&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;match&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;match&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;words&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;IsPerson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HasKeyword&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ListMoviesQuestion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;QuestionTemplate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;regex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Lemma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;list&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Lemma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;movie&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Lemma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;film&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;interpret&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;match&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;movie&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;IsMovie&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NameOf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;movie&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;enum&quot;&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;we can get the query output:&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;SELECT DISTINCT ?a WHERE &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
   ?a rdf:type dbpedia-owl:Film.
   ?x0 dbpprop:starring ?b.
   ?a foaf:name ?x2.
   ?b rdf:type &amp;lt;A&amp;gt;
   ?b rdfs:label &lt;span class=&quot;s2&quot;&gt;&quot;Audrey
   ?b rdfs:label &quot;&lt;/span&gt;Audrey
   &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;Problem that I think for II:
		We can see, the regex needs to be built from the parsing result.
		So, when dealing with a totally new question, it risks of disability to immediately figure 
		out the new regex for the new questions that the model has never faced before.
		How? With what? Based on what?
		I must go deeper.
		The regex is still not immediately created while facing a new genre of  question, this 
		needs  to be focused.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;summing-up-for-week-2&quot;&gt;Summing Up For Week 2&lt;/h2&gt;

&lt;p&gt;The Templates Generator component is basically built, 
however, still far from perfect.
The next days should delve into the automation about creating the new regexes for the never-processed genres of questions, for which I consider the semantic parsing is the point to break through.&lt;/p&gt;

&lt;p&gt;We have certain achievement this week, but still a long way to go.&lt;/p&gt;

</description>
        <pubDate>Sat, 08 Jun 2019 00:00:00 +0800</pubDate>
        <link>https://stuartchan.github.io/2019/06/08/Week-2/</link>
        <guid isPermaLink="true">https://stuartchan.github.io/2019/06/08/Week-2/</guid>
        
        <category>GSoC2019</category>
        
        <category>NLP</category>
        
        <category>DBpedia</category>
        
        
      </item>
    
      <item>
        <title>Architecture</title>
        <description>&lt;h1 id=&quot;the-pipeline-of-architecture&quot;&gt;The Pipeline of Architecture&lt;/h1&gt;

&lt;p&gt;The model architecture is like:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic3.zhimg.com/80/v2-a9408da7b2103f4018708efdc3e1f6be_hd.jpg&quot; alt=&quot;architecture and pipeline&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;pipeline&quot;&gt;Pipeline&lt;/h1&gt;

&lt;h2 id=&quot;1-question-generation-from-natural-language&quot;&gt;1. Question Generation from Natural Language&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://pic2.zhimg.com/80/v2-cbcf157a6a472066848f3623789a9565_hd.jpg&quot; alt=&quot;QuestionsGeneration&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We will use attention mechanism to generate questions from natural language contextual passage. By detecting entities in the passage, it will automatically get questions about the entities.&lt;/p&gt;

&lt;h3 id=&quot;11-input-from-natural-language&quot;&gt;1.1 Input from Natural Language&lt;/h3&gt;

&lt;p&gt;We use the natural language passage from Wikipedia as input :&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    Audrey Hepburn was a British actress and humanitarian. Recognised as a movie star and fashion icon, Hepburn was active during Hollywood's Golden Age. She was ranked by the American Film Institute as the third-greatest female screen legend in Golden Age Hollywood, and was inducted into the International Best Dressed List Hall of Fame. She rose to star in Roman Holiday in 1953.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;https://pic1.zhimg.com/80/v2-6a37b3b2f4db3949137d90642df08ff4_hd.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;12-attention-layer-with-embedding&quot;&gt;1.2 Attention Layer with Embedding&lt;/h3&gt;

&lt;p&gt;The passage goes through attention layer with embedding:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# T stands for time_steps, timing length
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;embedding_attention_seq2seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoder_inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# [T, batch_size]
&lt;/span&gt;                             	&lt;span class=&quot;n&quot;&gt;Decoder_inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# [T, batch_size]
&lt;/span&gt;                             	&lt;span class=&quot;n&quot;&gt;Cell&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                             	&lt;span class=&quot;n&quot;&gt;Num_encoder_symbols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                             	&lt;span class=&quot;n&quot;&gt;Num_decoder_symbols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                             	&lt;span class=&quot;n&quot;&gt;Embeddding_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                             	&lt;span class=&quot;n&quot;&gt;Num_heads&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# attention of the number of taps
&lt;/span&gt;                             	&lt;span class=&quot;n&quot;&gt;Output_projection&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#decoder's projection matrix
&lt;/span&gt;                             	&lt;span class=&quot;n&quot;&gt;Feed_previous&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                             	&lt;span class=&quot;n&quot;&gt;Dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                             	&lt;span class=&quot;n&quot;&gt;Scope&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                             	&lt;span class=&quot;n&quot;&gt;Initial_state_attention&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;encoder_cell&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rnn_cell&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;EmbeddingWrapper&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;embedding_classes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_encoder_symbols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;embeddding_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;embedding_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;encoder_outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoder_state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rnn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rnn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;encoder_cell&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoder_inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 

&lt;span class=&quot;n&quot;&gt;top_states&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
                  &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoder_outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;attention_states&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;top_states&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;embedding_attention_decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoder_inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                &lt;span class=&quot;n&quot;&gt;initial_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                &lt;span class=&quot;n&quot;&gt;attention_states&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                &lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                &lt;span class=&quot;n&quot;&gt;num_symbols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                &lt;span class=&quot;n&quot;&gt;embeddding_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                &lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                &lt;span class=&quot;n&quot;&gt;output_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                &lt;span class=&quot;n&quot;&gt;output_projection&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                &lt;span class=&quot;n&quot;&gt;feed_previous&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                &lt;span class=&quot;n&quot;&gt;update_embedding_for_previous&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                &lt;span class=&quot;n&quot;&gt;scope&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                &lt;span class=&quot;n&quot;&gt;initial_state_attention&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# core code
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;embedding&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;variable_scope&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;embedding&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                            &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_symbols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;embedding_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loop_function&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_extract_argmax_and_embed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_projection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;update_embedding_for_previous&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feed_previous&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;emb_inp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;embeddding_ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;embedding_lookup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decoder_inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# T * [batch_size, embedding_size]
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attention_decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;emb_inp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;initial_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;attention_states&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;output_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;loop_function&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loop_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;initial_state_attention&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;initial_state_attention&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;13-output-the-question&quot;&gt;1.3 Output The Question&lt;/h3&gt;
&lt;p&gt;Then we get the output question:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &quot;Which movie does Audrey Hepburn star ?&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;2-from-natural-language-questions-to-templates&quot;&gt;2. From Natural Language Questions to Templates&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://pic2.zhimg.com/80/v2-a23047e0556d185a03e86f145659d625_hd.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With the help of semantic parsing, the model extracts the templates from the questions.&lt;/p&gt;

&lt;h3 id=&quot;21-entity-annotation&quot;&gt;2.1 Entity Annotation&lt;/h3&gt;
&lt;p&gt;With the help of DBpedia’s application Spotlight, the model is able to detect the entities in the question:&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;se&quot;&gt;\e&lt;/span&gt;xamples&lt;span class=&quot;se&quot;&gt;\d&lt;/span&gt;bpedia&amp;gt;python annotation.py &lt;span class=&quot;s2&quot;&gt;&quot;Which movie does Audrey Hepburn star ?&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Audrey&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;Hepburn'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'@URI'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'http&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;//dbpedia.org/resource/Audrey_Hepburn'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Ref'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Audrey_Hepburn'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Schema'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Person'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'DBpedia'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Person'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Agent'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]}}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;the&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;is&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;('Which&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;movie&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;does&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&amp;lt;A&amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;star&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;?'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Person'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;22-template-generation-with-semantic-parsing&quot;&gt;2.2 Template Generation With Semantic Parsing&lt;/h3&gt;
&lt;p&gt;We deploy the method of semantic parser to get the core structure of a question for query generation:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;question&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Which movie does Audrey Hepburn star ?&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;parser&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;semantic_parser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;question&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;regex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parser&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;match_regex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;regex
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;Question&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Pos&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;DT&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt; + nouns&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Pos&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;NN&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; | Pos&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;NNS&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; | Pos&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;NNP&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; | Pos&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;NNPS&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;given thes above, the query template is generated:&lt;/p&gt;
&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;err&quot;&gt;'query'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;IsMovie()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;HasName(name)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;then, we can get the primitive query template:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;SELECT DISTINCT ?a WHERE {
   ?a rdf:type dbpedia-owl:Film.
   ?x0 dbpprop:starring ?b.
   ?a foaf:name ?x2.
   ?b rdf:type &amp;lt;A&amp;gt;
   ?b rdfs:label &quot;Audrey
   ?b rdfs:label &quot;Audrey}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;3-construction-of-the-templates-bank&quot;&gt;3. Construction of the Templates Bank&lt;/h2&gt;
&lt;p&gt;The Templates Bank is a base where stores the existed templates. 
The basic idea is to use the DBpedia entities embedding vectors to encode the question templates in order to match the existent templates.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic1.zhimg.com/80/v2-4a5f6b5e4f26dfb80083e9a3e3c337ec_hd.jpg&quot; alt=&quot;TemplatesMatching&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;4-comparison-of-newly-generated-templates-and-the-model-matching&quot;&gt;4. Comparison of Newly Generated Templates and the Model Matching&lt;/h2&gt;
&lt;p&gt;The templates generated in the previous step shall be compared against the tempalets in the Templates Bank as to match the similar one. Then, if matched, it will refer to the similar trained model that contains the matched template; if not matched, it turns to train the model on the new templates, and store the new results into the Templates Bank.&lt;/p&gt;
</description>
        <pubDate>Sun, 02 Jun 2019 00:00:00 +0800</pubDate>
        <link>https://stuartchan.github.io/2019/06/02/Architecture/</link>
        <guid isPermaLink="true">https://stuartchan.github.io/2019/06/02/Architecture/</guid>
        
        <category>GSoC2019</category>
        
        <category>NLP</category>
        
        <category>DBpedia</category>
        
        
      </item>
    
      <item>
        <title>Week 1</title>
        <description>&lt;blockquote&gt;
  &lt;h1 id=&quot;the-goals-for-week-1&quot;&gt;The Goals for Week 1&lt;/h1&gt;
&lt;/blockquote&gt;

&lt;p&gt;Let’s have a look into this week’s topics:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;
      &lt;p&gt;qald evaluation against GERBIL interface&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;building the semantic tree structue for the natural language sentence&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;the-balance-between-whether-to-write-a-script-to-de-the-evaluation-or-to-wrap-the-whole-system-into-gerbil&quot;&gt;The balance between whether to write a script to de the evaluation or to wrap the whole system into GERBIL&lt;/h2&gt;

&lt;h3 id=&quot;method-1-evaluationsh--evaluationpy-vs-mehod-2-wrapper-of-the-qa-system&quot;&gt;Method 1: evaluation.sh + evaluation.py V.S. Mehod 2: Wrapper of the QA System&lt;/h3&gt;

&lt;p&gt;For Method 1, the basic idea was that, based on the two files that I committed to my &lt;a href=&quot;https://github.com/StuartCHAN/neural-qa/tree/gsoc-stuart&quot;&gt;branch&lt;/a&gt;, the evaluation depends on running the GERBIL system on local machine to get a json-format response as comparison. 
It works on two parts parallel:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    1)first, in the evaluation.sh , it calls the NSpM model to give the output based on trained model;

    2)next, it retrieve the comparative output from the GERBIL to do the evaluation. The evaluation is conducted upon the data file after generation operation.  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For Method 2, it is that the GERBIL needs to wrap the whole self-built QA system that’s written in python into the &lt;a href=&quot;https://github.com/dice-group/GerbilQA-Benchmarking-Template/blob/master/src/main/java/org/dice/qa/impl/ExampleQASystem.java&quot;&gt;existed system&lt;/a&gt; that’s written in java via jython. And I was adviced that this would be more straightforward to spares us from implementing the evaluation module of our own.&lt;/p&gt;

&lt;p&gt;Besides, I have worked on cleaning up and refining a dataset from QALD that may be useful sooner, which contains a set of natural language queries and SPARQL queries with their answer’s value.&lt;/p&gt;

&lt;p&gt;When talking the question, those natural language queries, it highlights the importance of sparsing a sentence while generating the template.&lt;/p&gt;

&lt;h2 id=&quot;entity-recognition--semantic-parsing&quot;&gt;Entity Recognition &amp;amp; Semantic Parsing&lt;/h2&gt;

&lt;p&gt;To illustrate the ideas, I would hope to show you this figure that was extracted from the data below:&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;w&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                                &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;3&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                                &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;answertype&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;resource&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                                &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;aggregation&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                                &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;onlydbo&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                                &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;hybrid&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                                &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;question&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                                                &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;language&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;en&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                                                &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;string&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Who was the wife of U.S. president Lincoln?&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                                                &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;keywords&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;U.S. president, Lincoln, wife&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                                        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;…&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;query&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;sparql&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
        &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;PREFIX dbo: &amp;lt;http://dbpedia.org/ontology/&amp;gt;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;PREFIX res: &amp;lt;http://dbpedia.org/resource/&amp;gt;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;SELECT DISTINCT ?uri &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;WHERE {&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n\t&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;res:Abraham_Lincoln dbo:spouse ?uri.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;}&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                                &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                                &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;answers&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                                        &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;head&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                                                &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;vars&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                                                        &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;uri&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                                                &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                                        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                                        &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;results&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                                                &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;bindings&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                                                        &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;uri&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                                                                &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;uri&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                                                                &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;http://dbpedia.org/resource/Mary_Todd_Lincoln&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                                                        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;https://pic1.zhimg.com/80/v2-c9e0a1948e0e405081513620172f94f0_hd.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic3.zhimg.com/80/v2-bf0e8dcbe4a0a7804196c8fcfc055c62_hd.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This show the idelogical processing about how we detect the entities from the sentence and do the parsing, because we need to mapp them towards the RDF in DBpedia.&lt;/p&gt;

</description>
        <pubDate>Sat, 01 Jun 2019 00:00:00 +0800</pubDate>
        <link>https://stuartchan.github.io/2019/06/01/Week-1/</link>
        <guid isPermaLink="true">https://stuartchan.github.io/2019/06/01/Week-1/</guid>
        
        <category>GSoC2019</category>
        
        <category>NLP</category>
        
        <category>DBpedia</category>
        
        
      </item>
    
      <item>
        <title>Bonding</title>
        <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;What’s the template? If you ask me to describe it, I will imagine it as the bone of the question structure. If we have a question, like&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    What is the elephant lifesapn?
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;if we mark the word “elephant” as a variable placeholder “Species”, and mak the lifespan as the property “ageRange”, then what can we get?&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    dbo:Species;;;what is &amp;lt;A&amp;gt; lifespan;select ?a where { &amp;lt;A&amp;gt; dbo:ageRange ?a };select distinct(?a) where { ?a dbo:ageRange [] }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Yeah, here is the immitative syntactical silhouette of a question or sentence.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*QMgA0UViZMW7LUjRhil9yQ.jpeg&quot; alt=&quot;alt text&quot; title=&quot;DBpedia RDF&quot; /&gt;
&lt;img src=&quot;https://images2.minutemediacdn.com/image/upload/c_fill,g_auto,h_1248,w_2220/f_auto,q_auto,w_1100/v1555924214/shape/mentalfloss/451244723_0.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;the-templates-are-so-important&quot;&gt;The templates are so important!&lt;/h1&gt;

&lt;p&gt;It is the base that we get our training data in the NSpM model.&lt;/p&gt;

&lt;h2 id=&quot;ok-we-go-to-the-experiment-now&quot;&gt;OK, we go to the experiment now.&lt;/h2&gt;

&lt;p&gt;The templates about the &lt;a href=&quot;http://mappings.dbpedia.org/server/ontology/classes/Species&quot;&gt;ontology of species&lt;/a&gt; in DBpedia can be found &lt;a href=&quot;https://docs.google.com/spreadsheets/d/1o7mpc7TuJOBnMb4CmtC2FE1wsVrYUVMQR7fy0EOSSqo/edit?usp=sharing&quot;&gt;here&lt;/a&gt;, and these are the &lt;a href=&quot;https://drive.google.com/drive/folders/1J7olhKwObf4yMVaiO2vATixI2QnuZVY1?usp=sharing&quot;&gt;generated training data&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;experiments-records&quot;&gt;Experiments Records&lt;/h2&gt;

&lt;p&gt;I used the Species data that had been generated in previous step, and get the records like this:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Dataset&lt;/th&gt;
      &lt;th&gt;Examples per template&lt;/th&gt;
      &lt;th&gt;Training size&lt;/th&gt;
      &lt;th&gt;Average examples per instance&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;dbo:Species&lt;/td&gt;
      &lt;td&gt;39&lt;/td&gt;
      &lt;td&gt;12000&lt;/td&gt;
      &lt;td&gt;307.69&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The experimentation for dbo:Species was also conducted on a virtual cloud server with the GPU of GTX1080, 480GB SSD, and E5-2690 v3 24-core CPU of 64G, with basical setting for both comparative groups were Ubuntu 16 system with TensorFlow 1.3.0. The testing for the NMT model at was mainly displayed at three different times, i.e. at the 2,000th, 1,000th, and 12,000th iteration.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Dataset&lt;/th&gt;
      &lt;th&gt;BLEU 2k steps&lt;/th&gt;
      &lt;th&gt;BLEU 10k steps&lt;/th&gt;
      &lt;th&gt;BLEU 12k steps&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;dbo:Species&lt;/td&gt;
      &lt;td&gt;60.2 (May 25 07:24:53 2019)&lt;/td&gt;
      &lt;td&gt;84.3 (May 25 08:39:16 2019)&lt;/td&gt;
      &lt;td&gt;89.2 (May 25 08:54:56 2019)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;analyses&quot;&gt;Analyses&lt;/h2&gt;

&lt;p&gt;However, I saught a problem, you see the dev bleu and the test bleu don’t match 
&lt;img src=&quot;https://dbpedia.slack.com/files/UJA85N9G9/FJNK43M3M/image.png&quot; alt=&quot;alt text&quot; title=&quot;a screenshot when the 2,000th step&quot; /&gt;
What’s the problem?
The training has been interrupted a few times when the previous training steps displays the bleu dev is approaching 60 while the external bleu test records it still as 0. I have restarted the training several times, but it goes on to be like that. Maybe it was because the number of global steps was still below 2k steps? Here will show a piece of the record:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;External evaluation, global step 1000
decoding to output /data/DBPEDIA/neural-qa/data/annotations_Species/output/output_dev.
2019-05-25 07:20:16.906704: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
        [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?], [?]], output_types=[DT_INT32, DT_INT32], _device=&quot;/job:localhost/replica:0/task:0/cpu:0&quot;](Iterator)]]
2019-05-25 07:20:16.906708: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
        [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?], [?]], output_types=[DT_INT32, DT_INT32], _device=&quot;/job:localhost/replica:0/task:0/cpu:0&quot;](Iterator)]]
done, num sentences 1200, time 1s, Sat May 25 07:20:16 2019.
bleu dev: 59.3
saving hparams to /data/DBPEDIA/neural-qa/data/annotations_Species/output/hparams
External evaluation, global step 1000
decoding to output /data/DBPEDIA/neural-qa/data/annotations_Species/output/output_test.
2019-05-25 07:20:18.993003: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
        [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?], [?]], output_types=[DT_INT32, DT_INT32], _device=&quot;/job:localhost/replica:0/task:0/cpu:0&quot;](Iterator)]]
2019-05-25 07:20:18.993082: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
        [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?], [?]], output_types=[DT_INT32, DT_INT32], _device=&quot;/job:localhost/replica:0/task:0/cpu:0&quot;](Iterator)]]
done, num sentences 0, time 0s, Sat May 25 07:20:18 2019.
bleu test: 0.0
saving hparams to /data/DBPEDIA/neural-qa/data/annotations_Species/output/hparams
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;how-to-fix-the-zerodivisionerror&quot;&gt;How to fix the ZeroDivisionError?&lt;/h2&gt;
&lt;p&gt;We found that the log shows 0 sentences in test set, as a result this turns out to be zero.
So, we should change the argument values while spliting the data_.* files into train_., dev_., and test_.* .
It should trace back to the &lt;a href=&quot;https://github.com/AKSW/NSpM/blob/master/split_in_train_dev_test.py&quot;&gt;NSpM/split_in_train_dev_test.py&lt;/a&gt; file, and I think it was due to percentage that it set:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    TRAINING_PERCENTAGE = 90
    TEST_PERCENTAGE = 0
    DEV_PERCENTAGE = 10
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here we can see the developper tried to build the datasets based on the 10-fold cross-validation method. However, it is neglect about he situation where the number of total training templates might not be numerous enough.&lt;/p&gt;

&lt;p&gt;How to better the spliting?&lt;/p&gt;

&lt;p&gt;If the test set contains zero sentence, it is possible to try selecting 10 percent from the training data and 10 percent from the testing data to manually build one after generating and a check. The the ratio would be&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    TRAINING_PERCENTAGE : TEST_PERCENTAGE : DEV_PERCENTAGE = 90 : 10 : 10
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;with&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    TEST _fromTRAINING : TEST_fromDEV = 9 : 1
    TEST _fromTRAINING + TEST_fromDEV = TEST_PERCENTAGE
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;**We can see the training results &lt;a href=&quot;https://drive.google.com/drive/folders/1f2cs0Pz4-OmXUQ0nkr3RnOpBi7oE4NWB?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Sun, 26 May 2019 00:00:00 +0800</pubDate>
        <link>https://stuartchan.github.io/2019/05/26/Bonding/</link>
        <guid isPermaLink="true">https://stuartchan.github.io/2019/05/26/Bonding/</guid>
        
        <category>GSoC2019</category>
        
        <category>NLP</category>
        
        <category>DBpedia</category>
        
        
      </item>
    
      <item>
        <title>Think About The NSpM</title>
        <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;The NSpM model, Neural SPARQL Machines, are a type of LSTM-based Machine Translation Approaches for Question Answering based on external knowledge base in DBpedia and via SPARQL query.&lt;/p&gt;

&lt;p&gt;I feel touched by this model mostly because I have always the passion for the neural memory structure, the memorization mechanism and the reasoning processing, with which I saw a lucid hint from the NSpM.&lt;/p&gt;

&lt;p&gt;Now, let’s talk about it.&lt;/p&gt;

&lt;p&gt;Here is the code：
https://github.com/AKSW/NSpM&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.liberai.org/img/seq2seq-webexport-160px.png&quot; alt=&quot;alt text&quot; title=&quot;Neural SPARQL Machines&quot; /&gt;
&lt;img src=&quot;http://www.liberai.org/img/flag-sparql-160px.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;the-previous-training&quot;&gt;The Previous Training&lt;/h1&gt;

&lt;p&gt;I am an art fan, so I prefered the data with the topic ‘LC_QuAD_v6_art’. My lucky number is 608, then I randomly picked 608 temple from the file, and generated 98458 lines in the training data file. 
After running&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sh train.sh data/LC_QuAD_v6_art 120000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;different cases were found:&lt;/p&gt;

&lt;h2 id=&quot;trial-no1&quot;&gt;Trial No.1&lt;/h2&gt;
&lt;p&gt;At first, I choosed a laptop with Ubuntu 16, TensorFlow and Python 2.7 to do the computation, without GPU. It turned out that the training seemed abnormal while there were many conflicts in the codes, which kept printing that the modiles had something unmatched. Then I borrowed some functons with a same arithmetical meaning to replace the bugs. But the results were not so reasonable as expected, and my laptop’s Linux OS just collapesd.&lt;/p&gt;

&lt;h2 id=&quot;trial-no2&quot;&gt;Trial No.2&lt;/h2&gt;
&lt;p&gt;So it went to the next. That time I figured out that it might be due to the version of TensorFlow had not been satisfied. It had been deployed on a higher version than 1.3.0. So this time I was more fucuesd on the selection of the TensorFlow version. And considering about the hardware, the second trial was experimented on an Cloud ECS computing platform of &lt;a href=&quot;https://cn.aliyun.com/?accounttraceid=a3b99d73-db56-4cd2-ae2f-aa707c1e0a9e&quot;&gt;Alibaba Cloud&lt;/a&gt;. But there were still glitches. When I open the data holding folder, I found that the corresponding pars of natural language setences and the queries, repectively from the .en and .sparql files, were not well matched like the provided pretrained sample. And the final inferences did not feel so logic to the questions. It was strange-looking.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;We must be cautious about the following points:\
1, the different versions of TensorFlow can lead to the gap between the functions in the module:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tf.contrib
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;may be not existing any more in higher versions, which leads to the missing of its sub-modules.&lt;/p&gt;

&lt;p&gt;2, If it is run on the online computing cloud, must pay attention to the internet connection interface of virtual server, for it could be unable to use the SPARQL or any relevant functions to the DBpedia.&lt;/p&gt;

&lt;h1 id=&quot;recent-experiments&quot;&gt;Recent Experiments&lt;/h1&gt;

&lt;p&gt;Recently, the experiment have been into two trials:&lt;/p&gt;

&lt;h2 id=&quot;experiment-1&quot;&gt;Experiment 1&lt;/h2&gt;
&lt;p&gt;The experiment was still using the previously mentioned data from the topic ‘LC_QuAD_v6_art’. This trial was on a Lenovo computer without GPU, which had TensorFlow 1.3.0, Python 2.7, and tensorboard. The procedure was as the official guidance above. I checked into the files, where every thing seemed okay, and the inference after the training looked sensible.&lt;/p&gt;

&lt;p&gt;However, I did not notice the absence of the BLEU records. After rethinking about the experiment, I hypothesized that the early stop policy should be blamed, because I had set the stop, at about 12 00, less than the steps officially proposed.&lt;/p&gt;

&lt;p&gt;Another suspicion about the training is that, the originally provideded template contains a long number value at the last column which seems unexplainable.&lt;/p&gt;

&lt;h2 id=&quot;experiment-2&quot;&gt;Experiment 2&lt;/h2&gt;

&lt;p&gt;Ok, here is my fatal fight with this model.&lt;/p&gt;

&lt;p&gt;Firstly, it must have something to say with the template selection. I have konwn that any even subtle edition of the file can lead to a huge divergence. So, this time, I just try to be a good boy and use the very official provided template, checking all the required details to reproduce the experiment.&lt;/p&gt;

&lt;p&gt;Secondly, I admited that, at first, I had thought about(and physically, I really did the job) rewriting the cods, incuding overriding the lost function and the evaluation functions. But, anyway, I rewrote them back and do the suggested traditional way.&lt;/p&gt;

&lt;p&gt;Finally, I put it on a high performance GPU &lt;a href=&quot;https://www.jikecloud.net/list.html&quot;&gt;online cloud&lt;/a&gt;, sacrificing all the wages of my last week’s part-times to pay for the cloud device. I was excited facing the battle with the model.&lt;/p&gt;

&lt;p&gt;And, nice, look at what it brought us:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.google.com/document/d/1S49o0qlKtHYMHDekGryPbUO2CcVXHQeZ1m72Zdm4yVw/edit?usp=sharing&quot;&gt;Experiment 1 notes&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.google.com/document/d/1fkbylG4wK9waybCMiM8MKtrUFCUaartHKhShajt2UD0/edit?usp=sharing&quot;&gt;Experiment 2 notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments-results&quot;&gt;Experiments Results&lt;/h2&gt;

&lt;p&gt;I deployed the ‘LC_QuAD_v6_art’ dataset and the ‘monument_300’ dataset as parallel comparative groups to conduct the experiment. Here the evaluation of the accuracy of the neural model was based on the BLEU test, which is a modified precision metric for testing the machine translation output against the reference translation standard.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Dataset&lt;/th&gt;
      &lt;th&gt;Examples per template&lt;/th&gt;
      &lt;th&gt;Training size&lt;/th&gt;
      &lt;th&gt;Average examples per instance&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;dbo:LC_QuAD_v6_art&lt;/td&gt;
      &lt;td&gt;608&lt;/td&gt;
      &lt;td&gt;1200&lt;/td&gt;
      &lt;td&gt;161.94&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;dbo:Monument&lt;/td&gt;
      &lt;td&gt;300&lt;/td&gt;
      &lt;td&gt;12000&lt;/td&gt;
      &lt;td&gt;13.35 *&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Because of the pretraining of the dbo:Monument, the figure shares a great similarity this &lt;a href=&quot;https://arxiv.org/html/1708.07624&quot;&gt;report&lt;/a&gt;.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The experimentation for dbo:Monument was conducted on a virtual cloud server with the GPU of GTX1080, 480GB SSD, and E5-2690 v3 24-core CPU of 64G. The experimentation for dbo:LC_QuAD_v6_art was conducted on another virtual cloud server with the GTX1080, 2TB SSD, and E5-2658 v3 48-core CPU of 64G, and . The basical setting for both comparative groups were Ubuntu 16 system with TensorFlow 1.3.0. The testing for the NMT model at was mainly displayed at three different times, i.e. at the 3,000th, 9,000th, and 12,000th iteration.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Dataset&lt;/th&gt;
      &lt;th&gt;BLEU 3k steps&lt;/th&gt;
      &lt;th&gt;BLEU 9k steps&lt;/th&gt;
      &lt;th&gt;BLEU 12k steps&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;dbo:LC_QuAD_v6_art&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;dbo:Monument&lt;/td&gt;
      &lt;td&gt;67.0 (May 16 14:23:21 2019)&lt;/td&gt;
      &lt;td&gt;79.6 (May 16 15:55:34 2019)&lt;/td&gt;
      &lt;td&gt;80.2 (May 16 17:00:32 2019)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;final-analyses&quot;&gt;Final Analyses&lt;/h2&gt;

&lt;p&gt;In the different parts of the experiments, we can see the huge gap between the training results, which tells a lot of information.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The model may have a strong dependency on the generated data, which requires the high quality of the source template.&lt;/li&gt;
  &lt;li&gt;And, before all the training, make sure the environment is fit for every details of the prerequisites, e.g, the version of packages and the dependencies of them.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;p&gt;[1] Tommaso Soru, Edgard Marx, André Valdestilhas, Diego Esteves, Diego Moussallem, Gustavo Publio. (2018). Neural Machine Translation for Query Construction and Composition.&lt;/p&gt;

&lt;p&gt;[2] Papineni K., Roukos S., Ward T., Zhu W. (2002). BLEU: a method for automatic evaluation of machine translation. Proceedings of the 40th annual meeting on association for computational linguistics.&lt;/p&gt;
</description>
        <pubDate>Fri, 17 May 2019 00:00:00 +0800</pubDate>
        <link>https://stuartchan.github.io/2019/05/17/About-The-NSpM/</link>
        <guid isPermaLink="true">https://stuartchan.github.io/2019/05/17/About-The-NSpM/</guid>
        
        <category>GSoC2019</category>
        
        <category>NLP</category>
        
        <category>DBpedia</category>
        
        
      </item>
    
  </channel>
</rss>
