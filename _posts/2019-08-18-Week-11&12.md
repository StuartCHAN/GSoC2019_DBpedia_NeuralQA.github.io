---
layout:     post
title:      Week 11&12
subtitle:   Wiki Extraction & Sentences Filtering 
date:       2019-08-18
author:     Stuart Chen
header-img: img/meric-dagli-7NBO76G5JsE-unsplash.jpg
typo a-root-url: ..
catalog: true
tags:
    - GSoC2019
    - NLP
    - DBpedia
---


## Preface 

We focus on:

 1) get the natural language materials from the Wikipedia page for templates generation;

 2) automate the paraphrases for predicates.

------------------------------------------------------------------------

## 1. Wiki Extraction & Sentences Filtering

### 1.1 Wikipedia Articles Extraction

The Wiki pages are the excellent resources for us to get the natural language texts that containing the RDFs in DBpedia. 

We use the library called [Wikipedia-API](https://github.com/martin-majlis/Wikipedia-API) to use Python wrapper for Wikipediasâ€™ API, which facilitates extracting texts, sections, links, categories, translations, etc from the pages of Wikipedia. Documentation provides code snippets for the most common use cases.

For example, a 'dbr' should be like: 'Audrey_Hepburn', which is the identifier of both the DBPeida entity and the Wiki page.

```python

def extract_page(dbr, clas):
    # 
    #wiki_wiki = wikipediaapi.Wikipedia( language='en', extract_format=wikipediaapi.ExtractFormat.WIKI )    
    wiki_wiki = wikipediaapi.Wikipedia('en')
    page_wiki = wiki_wiki.page(dbr)
    #page_wiki = wiki_wiki.page('Audrey_Hepburn')    
    print(" Page - Exists: %s" % page_wiki.exists())
    # Page - Exists: True
    assert(page_wiki.exists())
    fulltext = page_wiki.text
    savepath = save_page(dbr, fulltext, clas)
    return savepath

```

### 1.2  Preprocesing

We then use the spaCy to do the sentences segmentation. When using a space-based pre-trained model, the sentences are split based on training data provided during the model's training procedure.

And, we also remove the puntuations except the comma in the sentences, because we need to use the comma to decompose the senetences in the part of converting them into questions.

Then we calculate the average length of the sentences we get and set it as the treshold to filter out those sentences that are too long.


## 2. Predicate-paraphrasing & RDF Matching

### 2.1. Coreference Resolution

Resolution of coreference is a well-studied problem in computer linguistics discourse. To obtain the correct interpretation of a text, or even to assess the relative significance of the different topics listed above, pronouns and other reference phrases must be related to the right individuals. Usually, algorithms designed to solve coreferences first look for the closestpre.

`Coreference Resolution in spaCy with Neural Networks.'

[NeuralCoref](https://github.com/huggingface/neuralcoref) is a spaCy 2.1 + pipeline extension that uses a neural network to annotate and resolve coreference clusters. NeuralCoref is ready for production, incorporated into the NLP pipeline of spaCy and can be extended to fresh training datasets.

It solves the problem of pinpointing the pronounces and indicating the right corefernce of an entity.


### 2.2. Automatic Predicate-paraphrasing

At first, we use the API of an online paraphraser to do the part of automatic paraphrasing, which we found that might not be suitable to do the paraphrasing of short phrases like the predicates.

Then, we use the wordnet via NLTK to take the paraphrasing task. 

```python
from nltk.corpus import wordnet
```

WordNet is the lexical database, equivalent to kind of English language dictionary, specifically intended for the processing of natural language. 

Synset is a unique kind of easy interface to look up phrases in WordNet that is present in NLTK. Synset instances are synonymous word groupings expressing the same concept. Some of the phrases have just one synset, while others have several.

```python

syn = wordnet.synsets('friend')[0]
print ("Synset name :  ", syn.name()) 

# Defining the word 
print ("\nSynset meaning : ", syn.definition()) 

# list of phrases that use the word in context 
print ("\nSynset example : ", syn.examples())
```

```bash
Synset name :   friend.n.01

Synset meaning :  a person you know well and regard with affection and trust

Synset example :  ['he was my best friend at the university']
```

To understand how the paraphraser works, we need to konw more about the `Hypernyms` and `Hyponyms` :

`Hypernyms` are the more abstract terms for the entity that a word describs.

`Hyponyms` are the specific terms, we can say that they are the subclasses of the entity.

```python

print ("\nSynset abstract term :  ", syn.hypernyms()[:5]) 

print ("\nSynset specific term :  ",  syn.hypernyms()[0].hyponyms()[:5]) 

syn.root_hypernyms() 
print ("\nSynset root hypernerm :  ", syn.root_hypernyms()[:5])
```

```bash
Synset name :   friend.n.01

Synset abstract term :   [Synset('person.n.01')]

Synset specific term :   [Synset('abator.n.01'), Synset('abjurer.n.01'), Synset('abomination.n.01'), Synset('abstainer.n.02'), Synset('achiever.n.01')]

Synset root hypernerm :   [Synset('entity.n.01')]
```

  What is the Hypernyms and Hyponyms distinction?

Hyponymy indicates the connection between a generic (hypernyme) word and a particular (hyponymous) term. 

A hyponym is a word or sentence that has a more particular semantic field than its hypernym. 

A hypernym's semantic field, also known as a superordinate, is wider than a hyponym's.

Then we can get:

```python   
ps = PorterStemmer() 

def paraphrase(word):
    word = str(word).strip().lower()
    synsets = wordnet.synsets(word)
    #synonyms = [[[lemma.name() for lemma in synonym.lemmas()] for synonym in synset.hyponyms()] for synset in synsets ]
    synonyms = [[lemma.name() for lemma in synonym.lemmas()] for synonym in synsets[0].hyponyms()]  
    synonymset = []
    for synonym in synonyms :
        for syn in synonym:
            syn = str(syn).replace('_', ' ').lower()
            if len(syn.split()) == 1:
                syn = ps.stem(syn)
            synonymset.append(syn) ;
    #synonymset.append(word)
    synonymset = list(set(synonymset))
    synonymset.insert(0, word)
    return synonymset

```

Then we can do the RDF matching, by given a triple of DBpedia RDF, 
 e.g. <http://dbpedia.org/resource/Barack_Obama>	<http://dbpedia.org/property/parents>	<http://dbpedia.org/resource/Barack_Obama_Sr.> .

we can have 

`{"subject":CoreferenceResolution(dbr_Barack_Obama), "predicate":dbp_parents+synonyms("parents"), "obeject":dbr_Barack_Obama_Sr}` 

to match the sentences that are sitable for this RDF triple.




## Referrences

[1] D Cer et al. 2018  Universal Sentence Encoder

[2] A Vaswani  2017  Attention Is All You Need

[3] Mohit Iyyer 2015 Deep Averaging Networks


