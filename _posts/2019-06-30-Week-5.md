---
layout:     post
title:      Week 5
subtitle:   Issues Analyses & Vectors Embedding
date:       2019-06-30
author:     Stuart Chen
header-img: img/justin-luebke-90718-unsplash.jpg
typo a-root-url: ..
catalog: true
tags:
    - GSoC2019
    - NLP
    - DBpedia
---


# The Week  5

This week I believe it's a diving.

I can now see many things, subtle and seemingly extending to somewhere abyssal.

In this week, I have tried transforming the natural language questions into SPARQL queries via the trained neural SPARQL machine models, which showed the restrictedness in the vocabulary mapping.

To trace back, I noticed that the Spotlight annotation might focus on the entity recognition based on the word-wise input. Thus, the entity consisting of more than one word could have been missed. 

Also, to make better use of the efforts of the existing templates, it is more efficient to vectorize the new question templates generated in the previous work in order to do the comparison with the existing question templates' embeddings.  

What's more, with the helpful advice, the project is moving forward an insightful and pioneering direction.



## 1. Issues Analyses

### 1.1 The Mismatching While Trained Models Work With Unprecedented Vocabulary

Case 1: Entities Ambiguation

	For example,
	when the NSpM model took an input natural question that contains
	"region",
	the model referred it to the "dbr:wineRegion" that it had only
	learned,
	which might be biased and divergent to the original sense.

Case 2: The Abused Mapping

	That's to say, when the model handles a vocabulary that is not in 
	the topic that it was trained, 
	it would mistakenly place the unprecedented word into the place of 
	another vocabulary that it had learned, then mismatching to the 
	value of the replaced vocabulary.

Case 3: Issue On Reproducibility

	The models that have attained BLEU could have not perfectly 
	translated the natural language questions with a new different 
	vocabulary into the correct query.


Summing Up:

	The current neural SPARQL machine would be having a strong 	
	dependency on the vocabulary that it have learned.
	Also, the present version of the model design has mostly focused on 
	the fitting of neural machine translation between the  natural 
	language questions and the SPARQL queries.


### 1.2 Expanding The Question:

	So, what should we do next?

I got some feedback from the researches in NL2SQL that major in deploying reinforcement learning method in training the neural model to get more and more correctness in the returned answers.

![Figure 1, SEQ2SQL(V Zhong et al.)](https://pic2.zhimg.com/80/v2-c45ddc6df9f5165326d871dcf7f31959_hd.jpg)

â€‹																		Figure 1, SEQ2SQL(V Zhong et al.)

The learning of the connection between the the natural language question and the structured query is the major objective function, where the  task is designed to let the query fit as much as possible during the machine translation training. 

While parallelly, the researches in NL2SQL have a comparative stronger emphasis on the result that the generated query would return and on the correctness that it would produce, because the correct return of the query is their only goal, not the matching similarity of the queries. 

That's to say, their models tend to build a more direct function from the natural language question through the generation of the structured query finally towards the return answer, based on which this type of algorithms of reinforcement learning get the reward upon the right or wrong of the result of the generated query then learning to let the query generator get the more and more pertinent returned answer. 

To sum up, the machine translation based-model is query-driven while the reinforcement learning model that tries to take a step further can be classified as a final-answer-driven method.

However, in this type of final-answer-driven models, there's a pre-requisite that requires to embed the natural language questions, the structured query, and the answers entities into a calculable form while working with our problem.



## 2. Vectors Embedding

	Then, why embed the data?

The problem of the restricted limitation of the vocabulary mapping, and the abused vocabulary matching, in my humble opinion, could be traced back to the restriction of the vocabulary that it was able to learning in the training data set.

So, how could we remove the limitation away from the current model?

There are two aspects to be paid attention:


>    1) Uniqueness and Directivity: The embedding vector must be unique in terms of 
>      representing the entities, otherwise there could be conflicting mapping. And it must be 
>      promising that, in the vector space, each entity-vector pair must be correctly matched in 
>      one unique key-value pair.       

>    2) Comprehensive Inclusion: The vector set should, comprehensively and accurately, 
>    comprise all the entities and relation properties in the DBpedia space, with inclusion in the 
>    embedding of the keywords of the query grammars, e.g. "SELECT","DISTINCT","WHERE".etc.

After the attempts in training a new vector set on the given templates, I figured out that it might be of more efficiency employing the DBpedia embedding vector of the [previous projects](https://github.com/dbpedia/embeddings).


##  References

[1] [Victor Zhong, Caiming Xiong, Richard Socher](https://paperswithcode.com/paper/seq2sql-generating-structured-queries-from)  (2017) SEQ2SQL: GENERATING STRUCTURED QUERIES
FROM NATURAL LANGUAGE USING REINFORCEMENT
LEARNING

[2] [Chen Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+C), [Mohammad Norouzi](https://arxiv.org/search/cs?searchtype=author&query=Norouzi%2C+M), [Jonathan Berant](https://arxiv.org/search/cs?searchtype=author&query=Berant%2C+J), [Quoc Le](https://arxiv.org/search/cs?searchtype=author&query=Le%2C+Q), [Ni Lao](https://arxiv.org/search/cs?searchtype=author&query=Lao%2C+N) (2018) Memory Augmented Policy Optimization for Program Synthesis and Semantic Parsing

